{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>NOTE</b>:<br> This notebook is my understanding of chapter 11 of `Hands-On Machine Learning with Scikit-Learn, Keras and Tensorflow 2nd Edition` book written by `Aurelien Geron`, entitled as `Training Deep Neural Networks`.<br>\n",
    "While most of the markdowns and code blocks have been copied from the chapter's notebook, the arraingment and changes made to the whole project is personalized based on my knowledge and what I comprehended and learned from the book. I'm going to use this notebook as a reference, when I want to apply Deep Neural Network on Machine Learning projects in the future.\n",
    "\n",
    "`Aurelien Geron`'s Github repository for the book can be found [here](https://github.com/ageron/handson-ml2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective\n",
    "\n",
    "In this chapter, we will go through some of deep learning problems in turn and present techniques to solve them. We will start by explaining the vanishing gradients problem and exploring some of the most popular solutions to this problem. Next, we will look at transfer learning and unsupervised pretraining, which can help you tackle complex tasks even when you have little labeled data. Then we will discuss various optimizers that can speed up training large models tremendously compared to plain Gradient Descent. Finally, we will go through a few popular regularization techniques for large neural networks.\n",
    "With these tools, you will be able to train very deep nets: welcome to Deep Learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tensorflow import keras\n",
    "from scipy.stats import reciprocal\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanishing/Exploding Gradients Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we know, the backpropagation algorithm works by going from the output layer to the input layer, propagating the error gradient on the way. Once the algorithm has computed the gradient of the cost function with regards to each\n",
    "parameter in the network, it uses these gradients to update each parameter with a Gradient Descent step.<br>\n",
    "\n",
    "Unfortunately, gradients often get smaller and smaller as the algorithm progresses down to the lower layers. As a result, the Gradient Descent update leaves the lower layer connection weights virtually unchanged, and training never converges to a good\n",
    "solution. This is called the <b>vanishing gradients</b> problem.<br>\n",
    "\n",
    "In some cases, the opposite can happen. Meaning the gradients can grow bigger and bigger, so many layers get insanely\n",
    "large weight updates and the algorithm diverges. This is the <b>exploding gradients</b> problem, which is mostly encountered in recurrent neural networks.<br>\n",
    "\n",
    "More generally, deep neural networks suffer from unstable gradients; different layers may learn at widely different speeds.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Function:\n",
    "\n",
    "The activation function is a mathematical gate between the input of a neuron and its output. It decides what to output, based on a set of rules it applies to the weighted sum of inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEJCAYAAACXCJy4AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Z1A+gAAAACXBIWXMAAAsTAAALEwEAmpwYAABJUUlEQVR4nO3dd3wUxfvA8c+kkJBCCwGkBqmG3hFUQlOa9N4FpSg2wIqFYkVEEEHxR1MRadKkBPiqEQRCFURapClVCBAgCak3vz/2EnJpJOSSvSTP+/XaV25353ae21yem8zNziqtNUIIIfI+J7MDEEIIkTMk4QshRD4hCV8IIfIJSfhCCJFPSMIXQoh8QhK+EELkE5Lw8zilVJBS6guz44CMxaKU+kspNTGHQkpa7yKl1PocqCdAKaWVUsVzoK4RSql/lVIWM85psliGKqXCzYxBgJJx+LmXUsoXmAR0AB4AwoC/gI+01lutZYoBsVrr22bFmSAjsSil/gJWaq0nZlMMAcCvgK/WOjTJ9sIYfw9hdqzrLPCF1npakm0FgGLAfzob//iUUkWBK8BYYCVwW2udIwlXKaWBXlrrlUm2FQS8tdZXciIGkToXswMQWfIj4AEMB04CJYAWgE9CAa31dXNCS8mRYklOa30zh+qJAS7nQFUVMP6+12utL+VAfenSWt8B7pgdR76ntZYlFy5AEUADbe5RLgijlZmwXhJYh/HH9w/wFMZ/BROTlNHAaGAtEAmEAC2BssBmIAI4CNRPVld34DAQDZwDJmD9LzKNWEpY60iIZVjyWFJ5PZWsz7lsjeMA0ClZmQLAB9ZjRgOngRcAP+trS7ossj5nEUZyBBgB/Ac4JzvuEmBdRuKwvlabuqzbA6zrxTNx3s4CbwFzgVvAeeCVdM7R0FRepx8wEfgrlbLhSdYnWn8HfYFTwG1gTdJ4reWGJIn5P+CbJLEmrfdsavVYt43EaKjEWH8+k2y/tv4uVljP8WlgoNl/e7l5kT783CvcunRWSrln4nnfYLT+WgFdgIHW9eTeApYCdYB91sfzgTlAPeAiRpIEQCnVAOMPcxVQC3gdeAMYk04si4DKQBugKzAYIzGlxwvYBLS1xvYjsEopVT3ZaxyM0Z3xEMZ/QGEYybSHtUwNjG6wF1OpYwVQ2FpHwuvzwjhfizMYR3eMxDzZWs8Dqb2YTJy3lzESbH3gY2CqUurh1I4JLAPaWR83ttZ9Lo2yqfED+gDdgMcxft/vJ4l5JMaHz0KgNkaX4l/W3Y2sP5+x1puwbkMp1Q34ApgB1ARmAnOUUk8mK/oOxgdrHevrWqCUKp+J1yKSMvsTR5b7XzCS13UgCtgFTAOaJCsThLVVDVTDaDU1TbK/HBBPyhb+h0nWa1q3jU2yLYAkLVXge+CXZHVPBM6nEUtV6/ObJ9lfIXksGTwPwcBb1sdVrMdtl0ZZm7iTbF+EtYVvXV8FfJdkfSBwE3DPSBzW9bPA+PTqz+B5Owv8kKzM30nrSiWWhtZ6/JIdNyMt/CigcJJtE4CTSdbPY3xPlFbdGuh5j3p2AAtS+R38ns770AXjP05p5d/nIi38XExr/SNQGngSo7XZDAhWSr2ZxlOqAxaMFnvCMc5htNaT+zPJ4/+sPw+nsq2E9edDGH/ESf0OlFFKFUrl+A9ZY9mTJJZ/0oglkVLKUyk1VSl1VCl1wzryoyGQ0OqrZz3ur+kdJwMWA12VUh7W9QHAj1rrqAzGkVEZPW9/Jitzkbvn3t7+0bbfaSTWpZQqAZQBfs5iHWm9bv9k2xJft9Y6DrhK9r3uPE8Sfi6ntY7SWm/VWk/WWjfD6HaZaB0NkhWxSatJZ1tG3kPpjUbJ7EiVaUAv4G2ML6jrYnxoZPX1JrcBiAO6WJNcG+525+RUHEnPTWwq+zL792sBVLJtrqmUs0dd9yv5+8HMWPIcOXF5z1GMf31T69c/jvE7b5CwQSlVFuO/hKw6BjRPtu0RjK6J1IZhJsTSOEks5TMQyyPAt1rrH7XWf2J0L1RKsv+g9bgt03h+jPWnc3qVaK2jMfrWB2D0Z1/G6JLKaBwJdaVbD5k/b1lxFSiplEqa9Otm5gDaGFZ5AWidTrFY7v91H81MPCJzJOHnUkopH6XUL0qpgUqp2kqpikqpXsCrwM9a61vJn6O1PoExyuYrpVRTpVRdjC/eIsl8Szu5T4EWSqmJSqmqSqkBwDhgamqFrbEEAnOVUg9bY1nEvYfuhQDdlFL1lVK1MFrdiR9uWusQYDkwTynVw3peHlVKDbIW+QfjtXZUSvlav4xNy2LgCWAURh+6JaNxWJ0FHlVKlUnnQqtMnbcsCsK4BuBNpVQlpdRwoOd9HOd94CWl1MvWmOsqpcYl2X8WaK2UKmW9HiA1nwCDlFLPKaWqKKWex/hwzY7XLawk4ede4RhfEr4I/AYcwRiKuASjRZqWoRit0SCM4ZnfY1ygE5WVYLTWBzC6OHpgvfjLuqR3Ze1Q4AzwC/CTNfaz96hqrDXe7RjfWwRbHyc12HqszzH+k1iEMeoGrfUF4F2MpPXfPeLbjtGa9ce2OyejcbyD8aX4KYzWdQr3ed7ui9b6GMZw2xEYfeNtMd4zmT3Ol8BzGCNx/sL44K6RpMg4jP+wzgF/pHGMNcDzGKOPjmK8j5/VWv+U2XhExsmVtvmcteV5Eehn/RJYCJFHyZW2+YxSqhXgjTHipgRGSzcUo5UmhMjD7NKlo5RaoJS6Yp0HJbX9A5RSfyqlDiuldiql6tijXnFfXIH3MBL+Txj9949prSNMjUoIke3s0qWjlHoMo0/5W611zVT2NwOOaa1vKKXaY1xY0yTLFQshhMgwu3TpaK23KaX80tm/M8lqMMacLEIIIXKQGX34wzFGNaSglBqBMYKAggULNihXrlxOxpUqi8WCk5MMZgI5FwnOnTuH1pry5WVKF8iZ90VodCjXY65T3K04xQoUy9a6ssIR/kZCQkJCtda+qe601xwNGBMu/XWPMi0xLrjwudfxGjRooB3Br7/+anYIDkPOhaFFixa6Tp06ZofhMLL7fRH4d6BmIvqZdc9oi8WSrXVllSP8jQD7dBp5Ncda+Eqp2sA8oL3W+lpO1SuEyN3aPNiGOR3m8HT9p7G9SFhkVo7872G9ZH4VMEgbV0IKIUS6jl09xoVbF3B2cmZ0o9G4Oqc27Y/IDLu08JVSP2BM+1pcKXUe40pGVwCt9VcYVxz6YMx3DRCntW5oj7qFEHnP5fDLtPu+HSU9S7L76d3SsrcTe43S6XeP/U8DT9ujLiFE3hYRE8GTPzxJaGQoq3qvkmRvR3KlrRDCYcRb4hmwagAHLh1gTZ81NCjd4N5PEhkmCV8I4TCm7ZzG2hNr+bzd5zxZLfndDkVWScIXQjiMkQ1HUsS9CCMbjjQ7lDxJrqIRQphu74W93Im9I8k+m0nCF0KY6o9Lf9Dym5a8FPiS2aHkeZLwhRCmOX/rPJ1+6ESxgsWYGDDR7HDyPOnDF0KY4lb0LTou6cjt6NvsGLaDB7wfMDukPE8SvhDCFKPWj+LIlSNsHLCRWiVrmR1OviAJXwhhindbvEvnap15vNLjZoeSb0gfvhAiRwWfD0ZrTbXi1ehbs6/Z4eQrkvCFEDnmx6M/8vD8h/l6/9dmh5IvScIXQuSI4PPBDFw9kIfLPszgOoPNDidfkoQvhMh2Z26cofMPnSntXZq1fddS0LWg2SHlS5LwhRDZKt4ST5elXYizxLGx/0Z8PVO/+57IfjJKRwiRrZydnJnadioFXQpSrXg1s8PJ1yThCyGyhdaa/Zf207B0Q9pVbmd2OALp0hFCZJMp26bQ+P8aE3w+2OxQhJUkfCGE3S3+czHvBr3LoDqDaFKmidnhCCtJ+EIIu9r2zzaGrxtOgF8A//fk/8ktCh2IJHwhhN1cDr9M16VdqVikIqt6r6KAcwGzQxJJyJe2Qgi7KelZksktJ9OhSgeKFixqdjgiGUn4Qogsi4qL4p+wf6hWvBpjGo8xOxyRBrt06SilFiilriil/kpjv1JKfa6UOqmU+lMpVd8e9QohzGfRFoauGUrT+U0JjQw1OxyRDnv14S8C0hto2x6oYl1GAF/aqV4hhMnmn5nPsiPLePORNynuUdzscEQ67NKlo7XeppTyS6dIF+BbrbUGgpVSRZRSD2itL6X1hBMnThAQEGCzrXfv3jz77LNERkbSoUOHFM8ZOnQoQ4cOJTQ0lJ49e6bYP3r0aPr06cO5c+cYNGhQiv3jxo3jySef5MSJE4wcadxIOSwsjCJFigDw1ltv0aZNGw4ePMhLL72U4vkffPABzZo1Y+fOnbz55psp9s+YMYO6devyv//9j/feey/F/rlz51KtWjV++uknPv300xT7v/vuO8qVK8eyZcv48suUn5krV66kePHiLFq0iEWLFqXYv3HjRjw8PJgzZw7Lly9PsT8oKAiAadOmsX79ept9BQsW5LXXXgNgypQp/Pzzzzb7fXx8+PHHHwF444032LVrl83+smXLsnjxYgBeeuklDh48aLO/atWqfP21MYPiiBEjCAkJsdlft25dZsyYAcDAgQM5f/68zf6HH36YDz/8EIAePXpw7do1m/2tW7fm7bffBqB9+/bcuXPHZn+nTp0YP348QIr3Hdi+9w4ePEhcXJxNuex47yXlqO+9Sw9cIqR6CENqDGF8s/HZ9t7btGkT4PjvvXfeeQcnJ9t2tD3fe/eT95LKqT78MsC5JOvnrdtsEr5SagTGfwC4uroSFhZmc5CQkBCCgoKIiopKsQ/g+PHjBAUFcfPmzVT3HzlyhKCgIK5cuZLq/sOHD+Pt7c2///6buD8+Pj7x8aFDh3BxceHkyZOpPv/AgQPExMTw119/pbp/3759hIWFcejQoVT37969m0uXLnH48OFU9+/atYtTp05x5MiRVPfv2LGDwoULc/z48VT3b9u2DXd3d0JCQlLdn/BHd+rUqRT779y5Q3h4OEFBQZw5cybFfovFkvj8pOcvgaura+L+8+fPp9h/8eLFxP0XL15Msf/8+fOJ+//7778U+//999/E/VevXuXWrVs2+8+cOZO4//r160RHR9vsP3XqVOL+1M5N0vdeXFwcWmubctnx3kvKEd97EUUjOFn1JF6XvehcpzO//fZbtr33EvY7+nsvLi6OyMhIm/33+97T2gmLxYMDB/5j8eLd3L4dy4UL5dDaDYulIBaLGxaLG0uXFmLfvpOEhcVw7Fh/4DfSooxGd9ZZW/jrtdY1U9m3HvhIa/27df1n4DWt9b60jtewYUO9b1+au3NMUFBQqp+6+ZGcC0NAQABhYWEpWor5TUx8DO9te48m8U3o2Kaj2eE4hIS/Ea0hPByuXYPr140l6eNbt4zl9m1jSe1xRMT9RqH2a60bprYnp1r4F4BySdbLWrcJIXKZy+GXcXVyxcfDh8ktJye2TvO66Gi4cgUuX7Zd/vvv7uNz5xoRFWUk9bi4rNfp7Q1eXuDhYSwFC979mfRx0m3vvpv28XIq4a8DxiillgJNgJvp9d8LIRxTREwEnZZ0wqIt7BuxDyeVN67d1BquXoV//oF//7VdErZdvZqRI3nefeQJxYqBj4/xM+lSuLCRzAsVsv2Z9LGnJzhl4vRu3bqVSpUqZX/CV0r9AAQAxZVS54F3AVcArfVXwEagA3ASiASeske9QoicE2+Jp/+q/vxx+Q/W9l2b65J9QlL/+28ICbn7MyQETp6EZN+lpuDsDCVLQqlStkvCtpIl4fTpPbRv35iiRcHNLWdeF8CcOXN47rnnEr88Tou9Run0u8d+DTxnj7qEEOYYt2Uc606s4/N2n9Opaiezw0nX7dvw119w+LDtcv162s8pWhQqVIDy5e8uSddLlbp3i1vrSEqVsu9rSb8+zZQpU/j4448B4wvi9MiVtkKIe1rwxwJm7p7Ji01e5Pkmz5sdjo3wcNi/H/bsgd27jcdnz6ZetnBhqFoVqlS5+zNhsY6+zjW01rzwwgssWLAgcWSQJHwhRJZ1rNKRNx95k8ktJ5sah9Zw4gRs22Yk9z174OhRsFhsyxUoAP7+UKuW7VK6NOSFyTvj4uIYNGgQ69atsxkGmvwageQk4Qsh0nT6xmnKFSpHSa+SvN/6/RyvX2ujf/3XXyEoyPh5+bJtGRcXqFcPmjSBxo2hYUOoVs3YnhdFRUXRtWtXtm/fnmLM/5UrV9J9bh49JUKIrDp38xyPLHiE9pXbM7/L/Byr9+ZN2LwZNmyAn3+GC8kGcJcoAQEB0KyZkeTr1gV39xwLz1S3b9+mTZs2HD58OMUVu5D6RYNJScIXQqRwK/oWHZd0JCI2gpcffjnb6wsJgZ9+MpL89u22Y9h9fIwE37KlsTz0UN7olsmsq1ev0qJFC06fPp3iat0Erq6uxMfHp5nXJeELIWzExsfSa0UvjoUeY2P/jdQskeLiebv4+29YvtxY/vzz7nZnZ3jsMejUCZ54AmrWzNx49Lzo3LlzNG/enMuXLxMbG5tmuQIFChAVFeWa1n5J+EIIG69sfYUtp7Yw78l5tK3U1q7HPn8eFi+GZcsg6cwUhQsbCT4hyReVe6ckOnv2LI0aNeLGjRvEx8dn5Clp3mZMEr4QwsaQOkMo7V2a4fWH2+V40dGwdi0sXAhbttwdUVOoEHTtCr17Q5s2OXuhUm5y69YtfHx8iIyMJCYmhrh05mywtv6lhS+ESN/pG6d5sOiD1HugHvUeqJfl4x0/DnPmwPff373gqUAB6NIFBg40WvKS5O+tdu3aHD9+nL/++ou5c+fy5ZdfptnSt36Rm2YLP5/3jAkhAILPB1NjTg1m75mdpeNYLLBxI7RrZ3y5OmuWkezr1oXPP4eLF40++86dJdlnVs2aNXnllVdwdU2zAZ8gzTMrLXwh8rnTN07T+YfOlPEuQ5+afe7rGBERMH++keBPnjS2FSwIgwbBqFHGOHmRdQsXLiT5lPZFihShePHiXLx4kaioKCwWi7TwhRAp3bhzg45LOhKv49k4YGOmb1F48yYsXlwePz948UUj2ZcvD1OnGl/Qzp0ryd5etNZ89dVXNkMy3dzceOGFF/j777/ZuXNnwt3SItM6hiR8IfIprTU9lvfg9I3TrOmzhqo+VTP83NBQePttY3Kx+fMfJDTUuAhq5Uo4dQpeecWYBljYz44dOwgPD0+xffhw48v1OnXqMGfOHLC9u6AN6dIRIp9SSjGq4Sierv80j1Z4NEPPuX0bpk2DTz+9e0emunVvMG1aUVq1yp8XROWUL7/8kohkt8GqU6cO5cuXz/AxJOELkQ/9e/NfyhcuT+8avTNUPiYGvv4aJk++eyOQdu3grbcgNvaQ3Poym0VERLB69Wqb/nsvLy/GjBmTqeNIl44Q+cziPxdTZVYVtv+z/Z5ltTYukvL3h+efN5J9s2bG9AebNkHz5jkQsODHH3/E2dnZZlt8fDw9evTI1HGkhS9EPvLb2d8YtnYYj5R/hCZlm6Rb9s8/4bnn4PffjfXq1eHDD41x9NJ1k7M+//xzm/57pRTdu3fHw8MjU8eRFr4Q+cSJ0BN0W9aNSsUq8WPvHyngnProvVu34OWXoX59I9mXKGF05xw+bFwZK8k+Z509e5YjR47YbPP09GT06NGZPpa08IXIB8KiwuiwpAMuTi5s6L+BogVTTlajNfzwA4wbZ8w57+QEY8bAlCm5725QecmCBQuwJLvDS6FChWjWrFmmjyUJX4h8oJBbIQbVHkS7yu14sOiDKfafOwcjRkBgoLH+8MMwe7aMoTebxWJh7ty5xMTEJG5zd3dn9OjRqPv4V0sSvhB5mEVbuBx+mdLepZkYMDHFfq1hwQIYO9boyilWDD75BIYOlSmJHUFqd7UCGDp06H0dT36lQuRhb/78JnW/qsvF2xdT7Dt3Djp0gKefNpJ9ly5w5AgMGybJ3lHMmTMnxdj7+vXrU7Zs2fs6nl1+rUqpdkqpE0qpk0qp11PZX14p9atS6g+l1J9KqQ72qFcIkbb/2/9/fLzjY3o81IMHvB6w2ff998aNRQIDjVb999/D6tVQqpRJwYoUwsPDWbdunc3Ye29vb55//vn7PmaWE75SyhmYDbQH/IF+Sin/ZMXeApZrresBfYE5Wa1XCJG2zSc3M3rDaNpVbsesDrMS+3vDw43umoEDbVv1/fvL6BtHs3HjxhRf1sbHx9O1a9f7PqY9WviNgZNa69Na6xhgKdAlWRkNFLI+Lgyk/P9SCGEXx64eo9eKXtQoUYNlPZfh4mR8VffHH9CgAXzzjTGT5bx50qp3ZO3bt+e9996jQoUKeHp64uzsTK9evXDPwh3bVfKpNjN9AKV6Au201k9b1wcBTbTWY5KUeQDYAhQFPIE2Wuv9qRxrBDACoGTJkg2WLl2apdjsITw8HC8vL7PDcAhyLgwvvfQS8fHxzJo1y+xQUnUn/g6zT81mSIUh+Lr5ojWsWlWGuXMrERvrRMWK4bzzzlH8/NKcVDFT5H1xV3acC601ISEhbNmyhS5dutxz7pyWLVvu11o3TPNgWVmAnsC8JOuDgC+SlRkLjLM+fhg4Cjild9wGDRpoR/Drr7+aHYLDkHNhaNGiha5Tp47ZYaQQHh2ub0Xdstl2+7bWPXpobYzH0XrUKK0jI+1br7wv7nKEcwHs02nkVXsMy7wAlEuyXta6LanhQDvrB8wupZQ7UBy4Yof6hcj34i3x9PuxHxdvXyT46WBcnFw4edK4MvbIEeP+sfPnQ8+eZkcqzGSPPvy9QBWlVEWlVAGML2XXJSvzL9AaQCn1EOAOXLVD3UIIYOzmsfwU8hNP1X0KFycXAgOhUSMj2VevDnv2SLIXdkj4Wus4YAywGTiGMRrniFJqslKqs7XYOOAZpdQh4AdgqPVfDyFEFn2++3M+3/M5Lzd9mWcbPceHHxrj68PCjHvH7t4N1aqZHaVwBHa50lZrvRHYmGzbO0keHwVkIlUh7GxDyAZe3vwy3ap3473HPmHAAGM+HIBJk4z56uUiKpFAplYQIherWaImA2sP5L2mX9LuCWe2bwcvL+NCqs6d7/18YT8BAQEULVrUoW8GI5/9QuRC1yKvYdEWKhSpwDu1vqH1Yx5s3w5lyhhTGueWZH/16lWeffZZ/Pz8cHNzo2TJkrRu3ZqtW7dm6PlBQUEopQgNDc3mSO9atGhRqkMvV61axTPPPJNjcdwPaeELkcvcjLpJwDcBNCnThOG+8+jc2bipeJ06sGGDkfRzix49ehAZGcn8+fOpXLkyV65c4bfffuPatWs5HktMTAwFCqR+j4CMKFasWKZvSJLTpIUvRC4SGx9LrxW9OB56nHIXXqBVKyPZt2tHYgs/twgLC2P79u189NFHtG7dmgoVKtCoUSPGjx9P3759AVi8eDGNGjXC29ubEiVK0KtXLy5cMEZ9nz17lpYtWwLg6+uLUipxFsmAgIAU93sdOnQonTp1SlwPCAhg9OjRjB8/Hl9fX5pb79c4ffp0ateujaenJ2XKlOHpp58mLCwMMP6jeOqpp4iIiEAphVKKiRMnJh5v5syZicf38/PjvffeY+TIkRQqVIiyZcvyySef2MQUEhJCixYtcHd3p1q1amzcuBEvLy8WLVpkl3OcnCR8IXIJrTXPbXyOrae3MjDuFyaPqU1UFIwcCT/9BN7eZkeYOV5eXnh5ebFu3TqioqJSLRMTE8OkSZM4dOgQ69evJzQ0lH79+gFQrlw5fvzxRwCOHDnCpUuXbBJuRixevBitNdu3b+fbb78FwMnJiRkzZnDkyBGWLFnCnj17Eicsa9asGTNmzMDDw4NLly5x6dIlxo8fn+bxP/vsM2rVqsWBAwd47bXXePXVV9m1axdgzHXfrVs3XFxcCA4OZtGiRUyaNIno6OhMvYZMSeuKLLMXudLW8ci5MJh1pe3U36dqJqJbj9yQeOXspElaWyw5HoqNrLwvVq5cqYsWLard3Nx006ZN9bhx43RwcHCa5Y8dO6YBfe7cucS6AX316lWbci1atNDPPfeczbYhQ4bojh072pSpVavWPWPctGmTLlCggI6Pj9daa71w4ULt6emZolyLFi10165dE9crVKig+/bta1OmcuXKesqUKVprrQMDA7Wzs7M+f/584v4dO3ZoQC9cuPCecaWFdK60lRa+ELlEkzJNqX90Ez/PNWYXnzkT3nknd89y2aNHDy5evMhPP/1E+/bt2blzJ02bNuWDDz4A4MCBA3Tp0oUKFSrg7e1Nw4bGFDH//vuvXepv0KBBim2//PILbdu2pWzZsnh7e9O9e3diYmK4fPlypo9fu3Ztm/XSpUtz5YoxwcDx48cpXbo0ZZL0wzVq1AinbBxHKwlfCAd3/c514uPhh48f5cDydjg7w7ffwgsvmB2Zfbi7u9O2bVveeecddu7cyfDhw5k4cSI3b97kiSeewMPDg++++469e/cSaL0HY9Jb/qXGycnJZh55gNjY2BTlPD09bdb/+ecfOnbsyEMPPcSKFSvYv38/CxYsyFCdqXF1dbVZV0qlmPI4J8koHSEc2Knrp2j69SNU/O039gZWxc0Nli/PPcMu74e/vz9xcXEcPHiQ0NBQPvjgAypWrAgYQx+TShhVEx8fb7Pd19eXS5cu2Ww7dOgQfn5+6da9b98+YmJi+Oyzz3B2dgZg/fr1KepMXt/9qF69OhcvXuTixYuULl06sf7s/ECQFr4QDur6net0+K4zt5Z+wd7Aqnh6wqZNeSfZX7t2jVatWrF48WL+/PNPzpw5w4oVK5g6dSqtW7fG398fNzc3vvjiC06fPs2GDRt4++23bY5RoUIFlFJs2LCBq1evEh4eDkCrVq3YtGkT69at48SJE4wdO5Zz587dM6YqVapgsViYMWMGZ86c4YcffmDGjBk2Zfz8/IiKimLr1q2Ehoames/ZjGjbti3VqlVjyJAhHDp0iODgYMaOHYuLi8t93aA8IyThC+GAouOi6bqkJyfnTSTmUA+8vWHzZrCOQswTvLy8aNq0KTNnzqRFixbUqFGDN998k/79+7Ns2TJ8fX355ptvWLNmDf7+/kyaNInp06fbHKNMmTJMmjSJCRMmULJkycShmMOGDUtcmjdvjre3N926dbtnTLVr12bmzJlMnz4df39/5s2bx7Rp02zKNGvWjFGjRtGvXz98fX2ZOnXqfb1+JycnVq9eTXR0NI0bN2bIkCFMmDABpVSWbnKSrrS+zTV7kVE6jkfOhSG7R+lYLBbdf/kQjf8yDVp7e2u9c2e2VZdl8r64K6vn4uDBgxrQ+/btu+9jkM3z4Qsh7CguTnH8q3fg6IMUKgRbtkCTJmZHJbLD6tWr8fT0pEqVKpw9e5axY8dSp04d6tevny31SZeOEA7kWvhN+vWDA788SOHCsHWrJPu87Pbt24wZMwZ/f38GDBjAQw89xObNm7OtD19a+EI4iJ9PBdG+5xViD/amcGH43/+gYep3JhV5xODBgxk8eHCO1SctfCEcwLGrx+nQ/yyxB3vj5aUJDJRkL+xPEr4QJvsv/ApNe+4iZs9Q3NwtrF+vaNrU7KhEXiQJXwgT3Ym9Q/0+G7m17SlcXC2sWe1EixZmRyXyKkn4Qpho+icFuLhxKE7OFlYsd6JdO7MjEnmZJHwhTDLryyjemuCMUvDdt0507Wp2RCKvk4QvhAlGfbKFF54zJtaaPRv69zc5IJEvSMIXIodN/X4Pc998DLQz77wbz+jRZkck8gu7JHylVDul1Aml1Eml1OtplOmtlDqqlDqilFpij3qFyG2W/S+E14ZXhzh3nh4Zw8R3nc0OSeQjWb7wSinlDMwG2gLngb1KqXVa66NJylQB3gCaa61vKKVKZLVeIXKbHQf/o3+3ohBdiE7dIvlqtkeuvnmJyH3s0cJvDJzUWp/WWscAS4Euyco8A8zWWt8A0FpfsUO9QuQaly7BwO7FsYT70vjR26z8wQNnadyLHGaPhF8GSDrR9HnrtqSqAlWVUjuUUsFKKRl8JvKN6zfiadfOwtkzzjRoAP/b4I2bm9lRifwop+bScQGqAAFAWWCbUqqW1josaSGl1AhgBEDJkiUJCgrKofDSFh4e7hBxOAI5F4awsDDi4+MzdC5iYxX9ny9G6IlalC0bwVtvHWT//pS32svN5H1xl6OfC3sk/AtAuSTrZa3bkjoP7NZaxwJnlFIhGB8Ae5MW0lp/DXwN0LBhQx0QEGCH8LImKCgIR4jDEci5MBQpUoSwsLB7ngutoXGHY4SeeAjPYrfYvr0Qfn7NcybIHCTvi7sc/VzYo0tnL1BFKVVRKVUA6AusS1ZmDUbrHqVUcYwuntN2qFsIh9Xn2RPsC3wIZ7c7/LrZi3vcTlWIbJflhK+1jgPGAJuBY8ByrfURpdRkpVTC3Tc3A9eUUkeBX4FXtNbXslq3EI7qrWlnWfFVNVDxLF+uaNRQLnkR5rNLH77WeiOwMdm2d5I81sBY6yJEnhYYCB+9XgGAT2ZE0L1zIZMjEsIgzQ4h7Ch4bzS9emni4xVvvAHjX5BkLxyHJHwh7OTUmVhaPB5OeLiif3947z2zIxLCliR8Iezgxg1No4ArxIT5UK3BZRYsACf56xIORt6SQmRRTAw0bPMvN/4tQ/HyV9i1tZRcWCUckiR8IbJAa2jT8yynD1TAvcgN9v7mS9GiZkclROok4QuRBe++C9t/8sPZ7Q4/B3rg5yezoQnHlVNTKwiR53z1dSxTprji5ARrf3SnWRNJ9sKxSQtfiPsQdqcxo0cbCX7OHOjYUZK9cHyS8IXIpNsRfvxzZhpYXBgw+hwjR5odkRAZIwlfiEy4cEFz8Nj7EFuIpk/8y7dflLv3k4RwEJLwhcig27ehYcBlLBFlcPXZw69rystYe5GryNtViAyIi4O+feHyyQdwLnSaKqVewd3d7KiEyBwZpSPEPWgNY56PZ+NGZ3x84MEH3yIm5qbZYQmRadLCF+IeXpv8H3O/cqaAm4W1a8HD46LN/sjISOrUqUO3bt2YOXMmu3bt4s6dOyZFK0TapIUvRDrmLQ7jk4klAfh0zjWaN/dNUaZgwYLExsayZs0aAgMDKVCgAJGRkZQrV46HH36YFi1a0KhRI2rWrImrq2tOvwQhEknCFyINv26PYsQwo6P+uTfOMWZY6iNylFJ88MEHDBo0iPDwcKKiogA4c+YMZ86cYc2aNTg7OxMVFUXlypV59NFHeeSRR2jevDkPPvhgjr0eISThC5GKv09aaNcxGh1bmMd7n2HW+xXTLd+5c2eKFClCeHh4in2RkZGJj48dO8axY8dYtGgRvr6+nD9/3u6xC5EW6cMXIplr14wrZ2NuF6Z60zNs+L4i6h4X0jo5OTF58mS8vLwyVIezszPfffedHaIVIuMk4QuRRHQ0dO2m+TtEUaeOZvdmP1wy+H/wgAEDcMvAvMgeHh68++67tGzZMovRCpE5kvCFsLJY4ImeF/l9u6LkA3GsX68oVCjjc+QUKFCAN998Ew8Pj3TLOTs7M2TIkKyGK0SmScIXwurpF6/w2/rSOLlFsGptDGXLZv4YI0eOxOkel9/euXOHGjVqEBwcfJ+RCnF/JOELAUz5JIyFX5QApzi+WXKHZo3Sb6WnxdPTkxdffBH3dC7DjYuL4/r167Rq1Yo5c+agtb7fsIXIFLskfKVUO6XUCaXUSaXU6+mU66GU0kqphvaoVwh7WLz0Du+8VgiASdMvMrB78Swd7+WXX0Yl+5Y3tQ+AO3fu8MorrzBgwIDEoZxCZKcsJ3yllDMwG2gP+AP9lFL+qZTzBl4Edme1TiHsZds2eHqoO2gnhrx8gndeLJ/lY/r4+PDUU09RoEABwPiStkuXLhQuXBhnZ2ebspGRkaxZs4Z69erx77//ZrluIdJjjxZ+Y+Ck1vq01joGWAp0SaXcFOBjQJoywiEcPqzp3FkTHa0YPVqz8NNqdjv2G2+8gZOTE66urjRq1IglS5Zw+PBhqlWrRsGCBW3K3rlzh7///ptatWrx888/2y0GIZKzR8IvA5xLsn7eui2RUqo+UE5rvcEO9QmRZefOwaOtw7l5U9GlaxyzZql7jrXPjLJly9KtWzd8fHxYvXo1Tk5OlCtXjv3799OjR48UI3ni4+O5desWTz75JO+//77064tsobL6xlJK9QTaaa2ftq4PApporcdY152AX4ChWuuzSqkgYLzWel8qxxoBjAAoWbJkg6VLl2YpNnsIDw/P8MU0eV1eORe3b7sw/NlqXD3vS6FKf7L0i+sUzMRUxy+99BLx8fHMmjUr3XKRkZFERUVRrFixFPt++uknZs+eTXR0dIp97u7u1K5dm3ffffeeQzwdQV55X9iDI5yLli1b7tdap/49qdY6SwvwMLA5yfobwBtJ1gsDocBZ6xIFXAQapnfcBg0aaEfw66+/mh2Cw8gL5yIyUuu6TW5p0LrgA6f0+f8iMn2MFi1a6Dp16mQ5lt27d2sfHx/t6uqqAZvFzc1Nly9fXh8/fjzL9WS3vPC+sBdHOBfAPp1GXrVHl85eoIpSqqJSqgDQF1iX5APlpta6uNbaT2vtBwQDnXUqLXwhslNsLHTqGsnB3d44F77E7794U6aEeS3oxo0bc/ToUerVq5eiJR8dHc25c+do0KABq1evNilCkddkOeFrreOAMcBm4BiwXGt9RCk1WSnVOavHF8Ie4uNh8GD4ZYsHzp43WL0+kvrVU051nNNKlCjBjh07GD58eIqkr7UmIiKCAQMG8MorrxAfH29SlCKvsMs4fK31Rq11Va11Ja31+9Zt72it16VSNkBa9yInaQ3PPqtZuhS8vWHHL4V48pFKZoeVyMXFhc8//5z58+en2md/584d5syZQ0BAANeuXTMhQpFXyJW2uUBAQABjxowxO4xc6403NF9/rXApEMu6dZomjZ3v/SQT9O3bl927d1O6dOkUk7BFRkaye/duatSowR9//GFShCK3y7MJ/+rVqzz77LP4+fnh5uZGyZIlad26NVu3bs3Q84OCglBKcfNmzt27dNGiRal+w79q1So+/PDDHIsjL/noI/j4YwVOsfScuIyAADuOvcwGNWvW5OjRozzyyCMpWvuxsbH8999/PPLII3zzzTcmRShyszyb8Hv06MGePXuYP38+ISEhrF+/nvbt25vyL3FMTEyWnl+sWDG8vb3tFE3+8dVX8MYbABaav/A1S14fYHZIGVK4cGG2bNnC+PHjU1ykBUZr/9lnn+WZZ57J8ntL5DNpDd8xe8nKsMwbN25oQG/dujXNMt99951u2LCh9vLy0r6+vrpnz576/PnzWmutz5w5k2KY3JAhQ7TWxpC85557zuZYQ4YM0R07dkxcb9GihR41apQeN26cLl68uG7YsKHWWutPP/1U16pVS3t4eOjSpUvr4cOH6xs3bmitjeFcyet89913U62zQoUKesqUKXrEiBHa29tblylTRk+dOtUmphMnTujHHntMu7m56apVq+oNGzZoT09PvXDhwvs5pYkx5hbff6+1UhYNWlcePE1HxUbZ7dj2GpaZERs2bNDe3t7ayckpxfvDw8ND16lTR1+4cCFHYklLbnpfZDdHOBdk87BMh+Pl5YWXlxfr1q1Lc1KqmJgYJk2axKFDh1i/fj2hoaH069cPgHLlyvHjjz8CsHDhQi5dusTMmTMzFcPixYvRWrN9+3a+/fZbwLgr0owZMzhy5AhLlixhz549PP/88wA0a9aMGTNm4OHhwaVLl7h06RLjx49P8/ifffYZtWrV4sCBA7z22mu8+uqr7Nq1CwCLxUK3bt1wcXEhODiYRYsWMWnSpFQv8smLli83RuRorSjV9TN2f/UUbi73vjGJI+rQoQN//PEHFStWTDEBW2RkJEeOHKFmzZr8/vvvJkUocpW0PgnMXrJ64dXKlSt10aJFtZubm27atKkeN26cDg4OTrP8sWPHNKDPnTuntb7b4l6zZo1NuYy28GvVqnXPGDdt2qQLFCig4+PjtdZaL1y4UHt6eqYol1oLv2/fvjZlKleurKdMmaK11jowMFA7Ozsn/seitdY7duzQQJ5v4a9YobWzs9Gyf+strWPjY+1eR0628BNERETo7t27aw8PjxQtfUAXLFhQf/bZZ9piseRoXFrnjvdFTnGEc0F+a+GD0Yd/8eJFfvrpJ9q3b8/OnTtp2rQpH3zwAQAHDhygS5cuVKhQAW9vbxo2NK5EtteMhQ0aNEix7ZdffqFt27aULVsWb29vunfvTkxMDJcvX8708WvXrm2zXrp0aa5cuQLA8ePHKV26NGXK3J3SqFGjRve8MUdut2oV9OuniY9XdH3mLyZPBhenDN6f0MF5eHiwcuVK3nvvvVT79e/cucOECRPo3bu3zU3ThUgqT2cAd3d32rZtyzvvvMPOnTsZPnw4EydO5ObNmzzxxBN4eHjw3XffsXfvXgIDA4F7f8Hq5OSUMGVEotjY2BTlPD09bdb/+ecfOnbsyEMPPcSKFSvYv38/CxYsyFCdqXF1dbVZV0phsVgyfZy8Ys0a6NNHExen4JEP6fP8EbtOhuYIlFK8/PLLbN68mSJFiuCS7Ga7kZGRrF+/njp16hAWFmZOkMKh5emEn5y/vz9xcXEcPHiQ0NBQPvjgAx577DGqV6+e2DpOkDCXefKrG319fbl06ZLNtkOHDt2z7n379hETE8Nnn33Gww8/TNWqVbl48WKKOu1xNWX16tW5ePGizfH37duXZz8Q1q2D3r0xkn3zj3n/fehbq4/ZYWWbRx99lCNHjuDv75+itR8VFcXt27dTzLsvBOTRhH/t2jVatWrF4sWL+fPPPzlz5gwrVqxg6tSptG7dGn9/f9zc3Pjiiy84ffo0GzZs4O2337Y5RoUKFVBKERwczNWrVwkPDwegVatWbNq0iXXr1nHixAnGjh3LuXPnUgvDRpUqVbBYLMyYMYMzZ87www8/MGPGDJsyfn5+REVFsXXrVkJDQ+/7X/O2bdtSrVo1hgwZwqFDhwgODmbs2LG4uLikuBNTbrd8OfToYcyTQ7NPGPbK37zxaJo3XcszSpcuzd69e+nXr5/NeH0PDw8CAwNlGK9IVZ5M+F5eXjRt2pSZM2fSokULatSowZtvvkn//v1ZtmwZvr6+fPPNN6xZswZ/f38mTZrE9OnTbY5RpkwZJk2axPz58ylZsmTila7Dhg1LXJo3b463tzfdunW7Z0y1a9dm5syZTJ8+HX9/f+bNm8e0adNsyjRr1oxRo0bRr18/fH19mTp16n29ficnJ1avXk10dDSNGzdmyJAhTJgwAaVUuvdazW0WLoR+/SAuDh7us43WI7fwVacv89yHWloKFCjA/PnzmTVrFgULFsTd3Z0vv/ySunXrmh2acFRpfZtr9iLTI9vXwYMHNaD37dt338dwpHMxa5bWxiw5Wk+ZorXFonVMXEyO1G3GKJ172b9/v541a5YpdTvS+8JsjnAuSGeUTt4YwiBSWL16NZ6enlSpUoWzZ88yduxY6tSpQ/369c0OLcs++ijhClqo0Hsm7YY1R6mGuDq7pv/EPKx+/fp54ncrslee7NIRcPv2bcaMGYO/vz8DBgzgoYceYvPmzbm6u0NrmDDBSPZKafwGvc/VOm+aHZYQuYa08POowYMHM3jwYLPDsJvYWBg50ui3d3bW1Bs9g/3F32Z199U0LJ363dyEELYk4QuHFx4OvXpBYCB4eMDjry1gjR7LZ098RpfqXcwOT4hcQ7p0hEP77z8ICDCSffHisPV/ccRUXsWYRmN4scmLZoeXp/j5+aUYOSbyFmnhC4cVEgLt2sGZM1CpEmzapKlSxYW1TdaiULn6+wizDB06lNDQUNavX59i3969e1NcIS7yllzdwt+2bRtTp04lJCTE7FCEnf36Kzz8sJHsGzaEr9f8xTM7WnLp9iVcnFxwdpIrSe3N19c31Vss5jSZ4z/75OqE//rrr/PWW29Rt25dypYty9ixY7lx44bZYYks+vJLePxxuH4dOnWCxWsvMGjLE5y6cQqNvvcBxH1J3qWjlOLrr7+mV69eeHp68uCDD7J48WKb51y4cIHJkydTtGhRihYtSseOHfn7778T9586dYouXbpQqlQpPD09qV+/for/Lvz8/Jg4cSLDhg2jSJEiDBiQO25Ukxvl2oR/69Yt9u/fT2xsLHfu3OHChQvMmjWL06dPmx2auE+xsTB6NDz7rHH17KuvwnfLbtNnXUduR99mQ/8NlPYubXaY+crkyZPp0qULhw4dok+fPgwbNixxRtnIyEhatmxJgQIF+O2339i1axcPPPAAbdq0SZwWJDw8nPbt27N161YOHTpEjx496N69O8ePH7epZ/r06VSvXp19+/Ylzmgr7C/XJvzNmzenuNGzp6cn9erVMykikRWhodC2rXFbQjc3+O47eP/DOAas6ctfV/5iRa8V1C5Z+94HEnY1aNAgBg4cSOXKlZkyZQouLi5s27YNgKVLl6K15rXXXqN27dpUr16duXPnEh4entiKr1OnDqNGjaJWrVpUrlyZCRMmUL9+fVauXGlTT4sWLXj11VepXLkyVapUyfHXmV/k2i9tlyxZwu3btxPXlVJ07tw5z8/5nhft3WvMdnn2LDzwgDHVcePG8F/4NU7fOM2cjnN4ovITZoeZLyW974KLiwu+vr6JM8vu37+fM2fO0KFDB5vZOSMjIzl16hQAERERTJo0ifXr13Pp0iViY2OJiopKcT+HhPtRiOxll4SvlGoHzAScgXla64+S7R8LPA3EAVeBYVrrf+63vtjYWLZs2WKzzdvbm759+97vIYUJtIbZs2HsWKM7p1EjWL0aEu7bUtKrJH+M/AN3l7wz4Vtuk959FywWC3Xr1uXll1+mSZMmNuWKFSsGwPjx4wkMDGTatGlUqVIFDw8PBg8enOKLWRkdlDOy3BxWSjkDs4H2gD/QTynln6zYH0BDrXVtYCVwf9NAWv3+++8pbv4QExNDq1atsnJYkYNu3oQ+feD5541k//zzsH27kexXH1vNwFUDiYqLkmTvwOrXr8/JkycpXLgwlStXtlkSEv7vv//O4MGD6dGjB7Vr16Zs2bKJrX+R8+zRwm8MnNRanwZQSi0FugBHEwporX9NUj4YGJiVCpcvX544P32CFi1a5Kmpf/OygweNK2dPngRvb5g/31gH2HNhDwNWDaB2ydop7iwm7OPWrVscPHjQZluRIkUyfZwBAwYwbdo0JkyYgLe3N+XLl+fcuXOsXbuWUaNGUaVKFapWrcrq1avp0qULrq6uTJo0iaioKPu8EJFp9kj4ZYCkdwA5DzRJoyzAcGBTajuUUiOAEQAlS5YkKCgoRRmtNUuXLrW5e1PBggWpV69equWzKjw8PFuOmxtl9VzEx8PKleWYP78isbFOVKoUzsSJR/D1vUNQEFyOusyzB56liEsRXiv/Grt37LZb7PYUFhZGfHx8rnxfXL58me3bt6cY3PDYY48RFRXFqVOnbF7XkSNHKF68eOJ68jIffvghc+bMoWvXrkRERODj40PdunU5evQoFy5coFevXnzyySc0b94cLy8vevbsib+/P5cvX048Rmr15lYOny/Smjc5owvQE6PfPmF9EPBFGmUHYrTw3e513LTmw//zzz+1p6enBhKXAgUK6NDQ0KxPJJ0KR5jf2lFk5VycPq31o4/encN+5EitIyPv7r9x54b2n+2vi3xURB+9cjTrwWYjR5wP30zyN3KXI5wLsnk+/AtAuSTrZa3bbCil2gATgBZa6+j7rWzVqlUpbhpeo0YNfHx87veQIhtpbXTZvPyyMQlaqVLGeocOtuVOXj9JaGQoq/us5iHfh8wJVog8zh4Jfy9QRSlVESPR9wX6Jy2glKoHzAXaaa2vpDxExi1ZssTmG353d3e5Ms9BnTtnXEi1YYOx3quXcRVtap/NDUs35PQLp/EsIKM1hMguWR6lo7WOA8YAm4FjwHKt9RGl1GSlVGdrsU8AL2CFUuqgUmrd/dR14cIF/vkn5WjOrl273lfsInvExcGMGfDQQ0ayL1IEvv8eli1Lmew/2P4BH/3+EVprSfZCZDO7jMPXWm8ENibb9k6Sx23sUc+6detsLvAAKFGiBJUqVbLH4YUd7NsHI0bAH38Y6z16wMyZd8fWJ7Xk8BIm/DKBgbWzNGhLCJFBueqy1MWLFyfO0QHGlX99+vQxMSKR4MYNeOEFaNLESPbly8NPP8HKlakn++3/bOeptU/xWIXHmPfkPJnqWIgckGsS/u3bt9m3b5/NNnd3d3r06GFSRAIgJsZowVeqBLNmgVLwyitw9Kgx02VqQq6F0HVZV/yK+LG6z2rcXNxSLyiEsKtcM5dOYGAgbm5uNl/YOjs706hRIxOjyr+0hrVrjeR+8qSxrVUrmD4d6tRJ/7l7LuyhgHMBNvbfSLGCxbI/WCEEkIsS/g8//GAzWRogk6WZ5Pff4a234LffjPVq1eCTT4wWfUZ6ZgbWHkiXal3wdvPO3kCFEDZyRbaMjY1l8+bNNtsKFSokk6XlsMOHC9GmDTz6qJHsfXyMbpzDh+HJJ9NP9hZt4el1T7M+xJg2V5K9EDkvVyT833//PcXonOjoaJksLYfs3GncgeqFF+rz889QqBC8+67RlTNmDCSbUDFVb//yNvP/mM+RK0eyP2AhRKocsktHKfVwrVq1EtdXrFhBRESETZnHHntMJkvLRvHxRh/99OmwY4exzdMzjnHjXHjpJShaNOPHWvDHAj74/QOeqf8MrzZ/NVviFULcm0MmfGDl4cOHqV69Ov3792fFihU2k6V5eXkxcKCM3c4O4eGwcKFx4VTC3SKLFDFa8o0aBdO58yOZOt7/Tv+PketH8nilx5ndYbYMvxTCRI6a8E8BpU+cOMH777+fojsnJiaGjh07mhNZHnX4MPzf/xm3FgwLM7Y9+CC89BI89RR4eUFQUFymj7vp701UL16d5T2X4+qcgb4fIUS2cdSEvxd4FEhxZxyAuLg4nnnmGfr3788TTzyBt7d8AXg/IiJg+XL4+msIDr67vVkzGDcOunSBZJ+1mTbt8Wncir5FYffCWTuQECLLHPVL20PpDbe0WCysXr2aYcOG4ePjw/fff5+DoeVucXGwdavRai9dGoYNM5J9oULw7LPGVbI7dkD37vef7CNjI+m9ojfHrh5DKSXJXggH4agt/KP3LmLcILlUqVIyWuceLBbYvRt++MGYwOxKkvlKH37YmPumVy+wx21F4y3xDFg1gLXH1zKw9kCZ6lgIB+KoCf9Y0i9pU6OUwsfHh507d/LAAw/kUFi5R3Q0/PKLMdJm3Tq4dOnuvipVYMAA6NcPqla1b72vbH2FNcfXMOOJGXSu1vneTxBC5BiHTPha6whXV1fi4tL+krBIkSLs2LGDChUq5GBkju3ff43umsBAY0l629+yZaF3b+jfH+rXz9gVsZk1e89sPgv+jOcbP8+LTV+0fwVCiCxxyIQPxsRoyW9UnqBQoUJs376dKlWq5HBUjuX6ddi+3UjyW7dCSIjt/jp1jC9eu3SBevWyJ8knsGgLK4+t5MmqT/LZE59lX0VCiPvmsAnfw8Mj1YTv6enJr7/+So0aNUyIyjxaG+Pid+ww5rLZscOYkTKpQoWgZUto2xY6dgQ/v5yLz0k5ETggkDhLHM5OWRzaI4TIFg6d8L28vGySvoeHB1u2bKF+/fomRpYzLl+GAweMUTP79xvTG/z3n20ZNzdo1AhatzamPmjcGFxy+Dd6/tZ5Xtn6CnM6zKFowaK4IVMdC+GoHDbhu7u728yE6eHhwdq1a2nWrJmJUdlfdDT8/TccOwZ//mkk+QMHjISfXPHi0Ly5sTzyiNEX72Zifr0VfYuOSzpy5sYZLjx6gaIFMzHfghAixzl0wk+4u1XBggVZtmwZbdrY5U6JOU5rI4GfOXM3uScsp08b89YkV6iQ0e+esDRtaoyucZSZCeIscfRZ2YcjV46wccBGapaoaXZIQoh7cNiE7+zsTNGiRbl58yYLFy6kU1q3T3IAFosxtv3iRTh71kjsCcvp08a2qKjUn+vkBJUrGzf8rlHDaLXXrw8VKxr7HJHWmuc3Pk/gyUC+7vQ1j1d63OyQhBAZ4LAJH6Bv3740aNDAlPvWag23b8PFi+7s2QOhoUZSv3QJLlwwknvCz8uXjStY0+PjYyTxBx80knvCUrUq5LZJP6/ducbGkxt5rflrPNPgGbPDEUJkkEMn/M8//zxLz4+PN5L2zZtw65bxM+njhJ9hYUZCT74Y0/g0zVBdPj7GVAXly99N7BUr3l0KFcrSS3EoxT2Kc2DEAemzFyKXsUvCV0q1A2YCzsA8rfVHyfa7Ad8CDYBrQB+t9dn0jhkWBkuXQmRk5peICCOZpzGMP8M8PcHLK4qyZd0pXtz40rR0aShTxviZ8LhUqdzXSr8fR28dZW3gWj55/BN8PHzMDkcIkUlZTvhKKWdgNtAWOA/sVUqt01onHSU+HLihta6slOoLfAyk209z6pRx6X9WeXtD4cJ3l0KFUq4XKUJiQk9YfHygYEEICgomICAg64HkcmdunOGtv96iqFdR3nrsLUn4QuRC9mjhNwZOaq1PAyillgJdsJ0ArQsw0fp4JfCFUkpprXVaB3V2DqdYsV9wdo7CySkaJ6conJ1T/+nkFJ1im4tLJM7OkShlW8WdO8aS2rDH1ISFhVGkSJGMFc6jYl1iOVj/INGu0VTeXpkeq3qYHZKpDh48SFxcnDQErORv5C5HPxf2SPhlgHNJ1s8DTdIqo7WOU0rdBHyA0KSFlFIjgBEArq6ulC49NsNBaG302ac2xDEr4uPjCUu4I0g+ZFEWzjQ/w52Cd/Db5kfMjRhiSHmPgvwkLi4OrXW+fl8kld//RpJy9HPhUF/aaq2/Br4GaNiwod63b5/JEUFQUFC+bsntu7iPFota8G2nbynbsmy+PhcJAgICCAsL4+DBg2aH4hDy+99IUo5wLtK7jag9RnpfAMolWS9r3ZZqGaWUC1AY48tb4eAalm7IqRdOMbC23ENYiNzOHgl/L1BFKVVRKVUA6AusS1ZmHTDE+rgn8Et6/ffCfEsOL2HuvrkAlPIqZXI0Qgh7yHLC11rHAWOAzcAxYLnW+ohSarJSKuEOGPMBH6XUSWAs8HpW6xXZZ/s/23lq7VP88NcPxFvs/KWIEMI0dunD11pvBDYm2/ZOksdRQC971CWyV8i1ELou60rFIhVZ1WeVTHUsRB7ioLO1CDNcjbhKh+874KSc2NB/A8UKFjM7JCGEHTnUKB1hrk0nN3Hx9kV+HvwzlYpVMjscIYSdScIXiQbXGUyriq0oW6is2aEIIbKBdOkIPtj+Adv+2QYgyV6IPEwSfj4378A8JvwygWV/LTM7FCFENpOEn49tPbWVUetH8Xilx5nRbobZ4Qghspkk/Hzqryt/0XNFT/x9/VnRawWuzq5mhySEyGaS8POpeQfm4enqyYb+GyjklofuziKESJMk/Hxq+hPTCX46mHKFy927sBAiT5CEn4/EW+IZv2U8Z8PO4qScKF+4vNkhCSFykCT8fGT8lvF8uutTtpzaYnYoQggTSMLPJ77Y8wUzds/gxSYvMqLBCLPDEUKYQBJ+PrA+ZD0vBr5Il2pd+PTxT80ORwhhEkn4eZzWmo93fEy9UvX4vvv3MvulEPmYzKWTxyml2DRgExExEXgW8DQ7HCGEiaSFn0fdir7FuM3jiIiJwKuAFyW9SpodkhDCZJLw86A4Sxx9VvZh5u6ZHLh0wOxwhBAOQrp08hitNWM2jiHwZCD/9+T/8WiFR80OSQjhIKSFn8dM2zmNufvn8nrz13m6/tNmhyOEcCCS8POQm1E3+XTXp/Su0Zv3W79vdjhCCAcjXTp5SGH3wux+ejclPEvgpOSzXAhhS7JCHnD6xmne3/Y+Fm2hQpEKFHQtaHZIQggHlKWEr5QqppTaqpT62/qzaCpl6iqldimljiil/lRK9clKncLWjTs36LikI5/u+pSLty+aHY4QwoFltYX/OvCz1roK8LN1PblIYLDWugbQDpihlCqSxXoFEBMfQ/fl3Tl1/RSr+6yW+9EKIdKV1YTfBfjG+vgboGvyAlrrEK3139bHF4ErgG8W6833tNY889MzBJ0NYkGXBbTwa2F2SEIIB6e01vf/ZKXCtNZFrI8VcCNhPY3yjTE+GGporS2p7B8BJEzlWA04cd/B2U9xINTsIByEnIu75FzcJefiLkc4FxW01qk2qu+Z8JVS/wNKpbJrAvBN0gSvlLqhtU7Rj2/d9wAQBAzRWgdnLG7zKaX2aa0bmh2HI5BzcZeci7vkXNzl6OfinsMytdZt0tqnlPpPKfWA1vqSNaFfSaNcIWADMCE3JXshhMhLstqHvw4YYn08BFibvIBSqgCwGvhWa70yi/UJIYS4T1lN+B8BbZVSfwNtrOsopRoqpeZZy/QGHgOGKqUOWpe6Waw3J31tdgAORM7FXXIu7pJzcZdDn4ssfWkrhBAi95ArbYUQIp+QhC+EEPmEJPxMUEqNU0pppVRxs2Mxi1LqE6XUces0Gavz21XTSql2SqkTSqmTSqnUrizPF5RS5ZRSvyqljlqnTXnR7JjMppRyVkr9oZRab3YsaZGEn0FKqXLA48C/Zsdisq1ATa11bSAEeMPkeHKMUsoZmA20B/yBfkopf3OjMk0cME5r7Q80BZ7Lx+ciwYvAMbODSI8k/Iz7DHgVyNffcmutt2it46yrwUB+msCnMXBSa31aax0DLMWYXiTf0Vpf0lofsD6+jZHoypgblXmUUmWBjsC8e5U1kyT8DFBKdQEuaK0PmR2LgxkGbDI7iBxUBjiXZP08+TjJJVBK+QH1gN0mh2KmGRgNwhRTxjgSuQGK1T2mkHgTozsnX0jvXGit11rLTMD4t/77nIxNOBallBfwI/CS1vqW2fGYQSnVCbiitd6vlAowOZx0ScK3SmsKCaVULaAicMiYH46ywAGlVGOt9eUcDDHHpDedBoBSaijQCWit89eFHBeAcknWy1q35UtKKVeMZP+91nqV2fGYqDnQWSnVAXAHCimlFmutB5ocVwpy4VUmKaXOAg211mbPiGcKpVQ7YDrQQmt91ex4cpJSygXji+rWGIl+L9Bfa33E1MBMYJ0d9xvgutb6JZPDcRjWFv54rXUnk0NJlfThi8z6AvAGtlqnyfjK7IByivXL6jHAZowvKZfnx2Rv1RwYBLRKMmVKB7ODEumTFr4QQuQT0sIXQoh8QhK+EELkE5LwhRAin5CEL4QQ+YQkfCGEyCck4QshRD4hCV8IIfKJ/wfxyu/uRF40kAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = np.linspace(-5, 5, 200)\n",
    "\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [1, 1], 'k--')\n",
    "plt.plot([0, 0], [-0.2, 1.2], 'k-')\n",
    "plt.plot([-5, 5], [-3/4, 7/4], 'g--')\n",
    "plt.plot(z, logit(z), \"b-\", linewidth=2)\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.annotate('Saturating', xytext=(3.5, 0.7), xy=(5, 1), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.annotate('Saturating', xytext=(-3.5, 0.3), xy=(-5, 0), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.annotate('Linear', xytext=(2, 0.2), xy=(0, 0.5), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.grid(True)\n",
    "plt.title(\"Sigmoid activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.2, 1.2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the logistic activation function (sigmoid), you can see that when inputs become large (negative or positive), the function saturates at 0 or 1, with a derivative extremely close to 0. Thus when backpropagation kicks in, it has virtually\n",
    "no gradient to propagate back through the network, and what little gradient exists keeps getting diluted as backpropagation progresses down through the top layers, so there is really nothing left for the lower layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution is that the connection weights of each layer must be initialized randomly as `fan-avg = (fan-in + fan-out)/2`, where fan-in is the number of inputs of a layer, and fan-out is the number of output of that layer. This initialization strategy is called Glorot initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Constant',\n",
       " 'GlorotNormal',\n",
       " 'GlorotUniform',\n",
       " 'HeNormal',\n",
       " 'HeUniform',\n",
       " 'Identity',\n",
       " 'Initializer',\n",
       " 'LecunNormal',\n",
       " 'LecunUniform',\n",
       " 'Ones',\n",
       " 'Orthogonal',\n",
       " 'RandomNormal',\n",
       " 'RandomUniform',\n",
       " 'TruncatedNormal',\n",
       " 'VarianceScaling',\n",
       " 'Zeros',\n",
       " 'constant',\n",
       " 'deserialize',\n",
       " 'get',\n",
       " 'glorot_normal',\n",
       " 'glorot_uniform',\n",
       " 'he_normal',\n",
       " 'he_uniform',\n",
       " 'identity',\n",
       " 'lecun_normal',\n",
       " 'lecun_uniform',\n",
       " 'ones',\n",
       " 'orthogonal',\n",
       " 'random_normal',\n",
       " 'random_uniform',\n",
       " 'serialize',\n",
       " 'truncated_normal',\n",
       " 'variance_scaling',\n",
       " 'zeros']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[name for name in dir(keras.initializers) if not name.startswith(\"_\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how we can change the initilization strategy while creating a layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x22af5230518>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.layers.Dense(10, activation=\"relu\", kernel_initializer=\"he_normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x22af6397f28>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init = keras.initializers.VarianceScaling(scale=2., mode='fan_avg',\n",
    "                                          distribution='uniform')\n",
    "keras.layers.Dense(10, activation=\"relu\", kernel_initializer=init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonsaturating Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that other activation functions behave much better that logistic activation functions in deep neural networks, in particular the ReLU activation function, mostly because it does not saturate for positive values (and also because it is quite fast to compute).<br>\n",
    "\n",
    "Unfortunately, the ReLU activation function is not perfect. It suffers from a problem known as the <b>dying ReLUs</b>: during training, some neurons effectively die, meaning they stop outputting anything other than 0. In some cases, you may find that half of your network’s neurons are dead, especially if you used a large learning rate. A neuron dies when its weights get tweaked in such a way that the weighted sum of its inputs are negative for all instances in the training set. When this happens, it just keeps outputting 0s, and gradient descent does not affect it anymore since the gradient of the ReLU function is 0 when its input is negative.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leaky ReLU\n",
    "\n",
    "To solve the dying ReLUs problem, you may want to use a variant of the ReLU function, such as the <b>leaky ReLU</b>. This function is defined as LeakyReLUα(z) = max(αz, z). The hyperparameter α defines how much the function “leaks”: it is the slope of the function for z < 0, and is typically set to 0.01. This small slope ensures that leaky ReLUs never die; they can go into a long coma, but they have a chance to eventually wake up. A 2015 paper compared several variants of the ReLU activation function and one of its conclusions was that the leaky variants always outperformed the strict ReLU activation function. In fact, setting α = 0.2 (huge leak) seemed to result in better performance than α = 0.01 (small leak). They also evaluated the randomized leaky ReLU (RReLU), where α is picked randomly in a given range during training, and it is fixed to an average value during testing. It also performed fairly well and seemed to act as a regularizer (reducing the risk of overfitting the training set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(z, alpha=0.01):\n",
    "    return np.maximum(alpha*z, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z, leaky_relu(z, 0.05), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([0, 0], [-0.5, 4.2], 'k-')\n",
    "plt.grid(True)\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.annotate('Leak', xytext=(-3.5, 0.5), xy=(-5, -0.2), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.title(\"Leaky ReLU activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.5, 4.2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['deserialize',\n",
       " 'elu',\n",
       " 'exponential',\n",
       " 'gelu',\n",
       " 'get',\n",
       " 'hard_sigmoid',\n",
       " 'linear',\n",
       " 'relu',\n",
       " 'selu',\n",
       " 'serialize',\n",
       " 'sigmoid',\n",
       " 'softmax',\n",
       " 'softplus',\n",
       " 'softsign',\n",
       " 'swish',\n",
       " 'tanh']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[m for m in dir(keras.activations) if not m.startswith(\"_\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, leaky ReLU can't be found in keras.activations. We have it as a keras.layer instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LeakyReLU', 'PReLU', 'ReLU', 'ThresholdedReLU']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[m for m in dir(keras.layers) if \"relu\" in m.lower()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train a neural network on Fashion MNIST using the Leaky ReLU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_set_X_full, train_set_Y_full), (test_set_X, test_set_Y) = keras.datasets.fashion_mnist.load_data()\n",
    "train_set_X_full = train_set_X_full / 255.0\n",
    "test_set_X = test_set_X / 255.0\n",
    "valid_set_X, train_set_X = train_set_X_full[:5000], train_set_X_full[5000:]\n",
    "valid_set_Y, train_set_Y = train_set_Y_full[:5000], train_set_Y_full[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.Dense(100, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 1.6314 - accuracy: 0.5054 - val_loss: 0.8886 - val_accuracy: 0.7160\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.8416 - accuracy: 0.7247 - val_loss: 0.7130 - val_accuracy: 0.7656\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.7053 - accuracy: 0.7637 - val_loss: 0.6427 - val_accuracy: 0.7898\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.6325 - accuracy: 0.7908 - val_loss: 0.5900 - val_accuracy: 0.8066\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.5992 - accuracy: 0.8021 - val_loss: 0.5582 - val_accuracy: 0.8198\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.5624 - accuracy: 0.8142 - val_loss: 0.5350 - val_accuracy: 0.8238\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.5379 - accuracy: 0.8218 - val_loss: 0.5156 - val_accuracy: 0.8304\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.5152 - accuracy: 0.8297 - val_loss: 0.5079 - val_accuracy: 0.8284\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.5100 - accuracy: 0.8269 - val_loss: 0.4895 - val_accuracy: 0.8388\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.4918 - accuracy: 0.8340 - val_loss: 0.4817 - val_accuracy: 0.8396\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_set_X, train_set_Y,\n",
    "                    epochs=10,\n",
    "                    validation_data=(valid_set_X, valid_set_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELU\n",
    "\n",
    "<b>Exponential linear unit (ELU)</b> looks a lot like the ReLU function, with a few major differences:<br>\n",
    "\n",
    "• First it takes on negative values when z < 0, which allows the unit to have an average output closer to 0. This helps alleviate the vanishing gradients problem, as discussed earlier. The hyperparameter α defines the value that the ELU function\n",
    "approaches when z is a large negative number. It is usually set to 1, but you can tweak it like any other hyperparameter if you want.<br>\n",
    "\n",
    "• Second, it has a nonzero gradient for z < 0, which avoids the dead neurons problem.<br>\n",
    "\n",
    "• Third, if α is equal to 1 then the function is smooth everywhere, including around z = 0, which helps speed up Gradient Descent, since it does not bounce as much left and right of z = 0.<br>\n",
    "\n",
    "The main drawback of the ELU activation function is that it is slower to compute than the ReLU and its variants (due to the use of the exponential function), but during training this is compensated by the faster convergence rate. However, at test time\n",
    "an ELU network will be slower than a ReLU network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elu(z, alpha=1):\n",
    "    return np.where(z < 0, alpha * (np.exp(z) - 1), z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z, elu(z), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [-1, -1], 'k--')\n",
    "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(r\"ELU activation function ($\\alpha=1$)\", fontsize=14)\n",
    "plt.axis([-5, 5, -2.2, 3.2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing ELU in TensorFlow is trivial, just specify the activation function when building each layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"elu\"),\n",
    "    keras.layers.Dense(100, activation=\"elu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 1.4762 - accuracy: 0.5483 - val_loss: 0.8035 - val_accuracy: 0.7408\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.7773 - accuracy: 0.7456 - val_loss: 0.6665 - val_accuracy: 0.7828\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.6650 - accuracy: 0.7784 - val_loss: 0.6104 - val_accuracy: 0.8002\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.6060 - accuracy: 0.7995 - val_loss: 0.5666 - val_accuracy: 0.8100\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.5789 - accuracy: 0.8053 - val_loss: 0.5397 - val_accuracy: 0.8176\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.5472 - accuracy: 0.8145 - val_loss: 0.5202 - val_accuracy: 0.8226\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.5266 - accuracy: 0.8196 - val_loss: 0.5043 - val_accuracy: 0.8294\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.5084 - accuracy: 0.8268 - val_loss: 0.4965 - val_accuracy: 0.8310\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.5050 - accuracy: 0.8256 - val_loss: 0.4825 - val_accuracy: 0.8374\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.4888 - accuracy: 0.8313 - val_loss: 0.4756 - val_accuracy: 0.8394\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_set_X, train_set_Y,\n",
    "                    epochs=10,\n",
    "                    validation_data=(valid_set_X, valid_set_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SELU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Scaled version of ELU (SELU)</b> activation function was proposed in this [great paper](https://arxiv.org/pdf/1706.02515.pdf) by Günter Klambauer, Thomas Unterthiner and Andreas Mayr, published in June 2017. During training, a neural network composed exclusively of a stack of dense layers using the SELU activation function and LeCun initialization will self-normalize: the output of each layer will tend to preserve the same mean and variance during training, which solves the vanishing/exploding gradients problem. As a result, this activation function outperforms the other activation functions very significantly for such neural nets, so you should really try it out. Unfortunately, the self-normalizing property of the SELU activation function is easily broken: you cannot use ℓ<sub>1</sub> or ℓ<sub>2</sub> regularization, regular dropout, max-norm, skip connections or other non-sequential topologies (so recurrent neural networks won't self-normalize). However, in practice it works quite well with sequential CNNs. If you break self-normalization, SELU will not necessarily outperform other activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import erfc\n",
    "\n",
    "# alpha and scale to self normalize with mean 0 and standard deviation 1\n",
    "alpha_0_1 = -np.sqrt(2 / np.pi) / (erfc(1/np.sqrt(2)) * np.exp(1/2) - 1)\n",
    "scale_0_1 = (1 - erfc(1 / np.sqrt(2)) * np.sqrt(np.e)) * np.sqrt(2 * np.pi) * (2 * erfc(np.sqrt(2))*np.e**2 + np.pi*erfc(1/np.sqrt(2))**2*np.e - 2*(2+np.pi)*erfc(1/np.sqrt(2))*np.sqrt(np.e)+np.pi+2)**(-1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selu(z, scale=scale_0_1, alpha=alpha_0_1):\n",
    "    return scale * elu(z, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z, selu(z), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [-1.758, -1.758], 'k--')\n",
    "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(\"SELU activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -2.2, 3.2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the SELU hyperparameters (`scale` and `alpha`) are tuned in such a way that the mean output of each neuron remains close to 0, and the standard deviation remains close to 1 (assuming the inputs are standardized with mean 0 and standard deviation 1 too). Using this activation function, even a 1,000 layer deep neural network preserves roughly mean 0 and standard deviation 1 across all layers, avoiding the exploding/vanishing gradients problem:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a neural net for Fashion MNIST with 100 hidden layers, using the SELU activation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "model.add(keras.layers.Dense(300, activation=\"selu\",\n",
    "                             kernel_initializer=\"lecun_normal\"))\n",
    "for layer in range(99):\n",
    "    model.add(keras.layers.Dense(100, activation=\"selu\",\n",
    "                                 kernel_initializer=\"lecun_normal\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train it. Do not forget to scale the inputs to mean 0 and standard deviation 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_means = train_set_X.mean(axis=0, keepdims=True)\n",
    "pixel_stds = train_set_X.std(axis=0, keepdims=True)\n",
    "train_set_X_scaled = (train_set_X - pixel_means) / pixel_stds\n",
    "valid_set_X_scaled = (valid_set_X - pixel_means) / pixel_stds\n",
    "test_set_X_scaled = (test_set_X - pixel_means) / pixel_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1719/1719 [==============================] - 36s 19ms/step - loss: 1.3906 - accuracy: 0.4592 - val_loss: 0.7322 - val_accuracy: 0.7440\n",
      "Epoch 2/5\n",
      "1719/1719 [==============================] - 37s 21ms/step - loss: 0.6896 - accuracy: 0.7526 - val_loss: 0.6524 - val_accuracy: 0.7696\n",
      "Epoch 3/5\n",
      "1719/1719 [==============================] - 30s 18ms/step - loss: 0.5689 - accuracy: 0.8008 - val_loss: 0.5594 - val_accuracy: 0.7982\n",
      "Epoch 4/5\n",
      "1719/1719 [==============================] - 34s 20ms/step - loss: 0.5811 - accuracy: 0.8027 - val_loss: 0.4854 - val_accuracy: 0.8346\n",
      "Epoch 5/5\n",
      "1719/1719 [==============================] - 28s 16ms/step - loss: 0.5068 - accuracy: 0.8239 - val_loss: 0.4546 - val_accuracy: 0.8454\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_set_X_scaled, train_set_Y,\n",
    "                    epochs=5,\n",
    "                    validation_data=(valid_set_X_scaled, valid_set_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now look at what happens if we try to use the ReLU activation function on scaled train and valid asets instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "model.add(keras.layers.Dense(300, activation=\"relu\", kernel_initializer=\"he_normal\"))\n",
    "for layer in range(99):\n",
    "    model.add(keras.layers.Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1719/1719 [==============================] - 24s 13ms/step - loss: 2.0875 - accuracy: 0.1791 - val_loss: 1.4671 - val_accuracy: 0.3808\n",
      "Epoch 2/5\n",
      "1719/1719 [==============================] - 23s 13ms/step - loss: 1.3122 - accuracy: 0.4300 - val_loss: 1.0152 - val_accuracy: 0.5714\n",
      "Epoch 3/5\n",
      "1719/1719 [==============================] - 20s 12ms/step - loss: 1.0237 - accuracy: 0.5732 - val_loss: 0.9137 - val_accuracy: 0.5864\n",
      "Epoch 4/5\n",
      "1719/1719 [==============================] - 19s 11ms/step - loss: 0.8591 - accuracy: 0.6500 - val_loss: 0.7524 - val_accuracy: 0.7178\n",
      "Epoch 5/5\n",
      "1719/1719 [==============================] - 19s 11ms/step - loss: 0.8111 - accuracy: 0.6780 - val_loss: 0.6833 - val_accuracy: 0.7482\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_set_X_scaled, train_set_Y,\n",
    "                    epochs=5,\n",
    "                    validation_data=(valid_set_X_scaled, valid_set_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not great at all, we suffered from the vanishing/exploding gradients problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, they also evaluated the <b>parametric leaky ReLU (PReLU)</b>, where α is authorized\n",
    "to be learned during training (instead of being a hyperparameter, it becomes a\n",
    "parameter that can be modified by backpropagation like any other parameter). This\n",
    "was reported to strongly outperform ReLU on large image datasets, but on smaller\n",
    "datasets it runs the risk of overfitting the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.PReLU(),\n",
    "    keras.layers.Dense(100, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.PReLU(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 1.6969 - accuracy: 0.4974 - val_loss: 0.9255 - val_accuracy: 0.7186\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.8706 - accuracy: 0.7247 - val_loss: 0.7305 - val_accuracy: 0.7628\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.7211 - accuracy: 0.7620 - val_loss: 0.6565 - val_accuracy: 0.7878\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.6448 - accuracy: 0.7881 - val_loss: 0.6004 - val_accuracy: 0.8046\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.6078 - accuracy: 0.8004 - val_loss: 0.5656 - val_accuracy: 0.8182\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.5693 - accuracy: 0.8118 - val_loss: 0.5406 - val_accuracy: 0.8238\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.5428 - accuracy: 0.8193 - val_loss: 0.5196 - val_accuracy: 0.8312\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.5193 - accuracy: 0.8283 - val_loss: 0.5113 - val_accuracy: 0.8320\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.5128 - accuracy: 0.8273 - val_loss: 0.4916 - val_accuracy: 0.8380\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4941 - accuracy: 0.8313 - val_loss: 0.4826 - val_accuracy: 0.8396\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_set_X, train_set_Y,\n",
    "                    epochs=10,\n",
    "                    validation_data=(valid_set_X, valid_set_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing an Activation Function For Hidden Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So which activation function should you use for the hidden layersvof your deep neural networks?<br>\n",
    "Although your mileage will vary, in general<br>\n",
    "\n",
    "`SELU > ELU > leaky ReLU (and its variants) > ReLU > tanh > logistic`<br>\n",
    "\n",
    "If the network’s architecture prevents it from self-normalizing, then ELU may perform better than SELU (since SELU is not smooth at z = 0). If you care a lot about runtime latency, then you may prefer leaky ReLU. If you don’t want to tweak yet another\n",
    "hyperparameter, you may just use the default α values used by Keras (e.g., 0.3 for the leaky ReLU). If you have spare time and\n",
    "computing power, you can use cross-validation to evaluate other activation functions, in particular RReLU if your network is overfitting, or PReLU if you have a huge training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization\n",
    "\n",
    "Although using He initialization along with ELU (or any variant of ReLU) can significantly reduce the vanishing/exploding gradients problems at the beginning of training, it doesn’t guarantee that they won’t come back during training.\n",
    "<b>Batch Normalization (BN)</b> is a technique to address the vanishing/exploding gradients problems.\n",
    "The technique consists of adding an operation in the model just before or after the activation function of each hidden layer, simply zero-centering and normalizing each input, then scaling and shifting the result using four new parameter.<br>\n",
    "\n",
    "Four parameter vectors which are learned in each batch-normalized\n",
    "layer are:<br>\n",
    "γ (the ouput scale vector)<br>\n",
    "β (the output offset vector) are learned through regular backpropagation<br>\n",
    "μ (the final input mean vector)<br>\n",
    "σ (the final input standard deviation vector) are estimated using an exponential moving average.<br>\n",
    "\n",
    "Note that μ and σ are estimated during training, but they are not used at all during training,\n",
    "only after training (to replace the batch input means and standard deviations).<br>\n",
    "\n",
    "In other words, this operation lets the model learn the optimal scale and mean of each of the layer’s inputs. In many cases, if you add a BN layer as the very first layer of your neural network, you do not need to standardize your training set (e.g., using a StandardScaler): the BN layer will do it for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, activation=\"relu\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_5 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "dense_213 (Dense)            (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "dense_214 (Dense)            (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_215 (Dense)            (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 271,346\n",
      "Trainable params: 268,978\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each BN layer adds 4 parameters per input: γ, β, μ and σ (for example, the first BN layer adds 3136 parameters, which is 4 times 784). The last two parameters, μ and σ, are the moving averages, they are not affected by backpropagation, so Keras calls them “Nontrainable” (if you count the total number of BN parameters, 3136 + 1200 + 400, and divide by two, you get 2,368, which is the total number of non-trainable params in this model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('batch_normalization/gamma:0', True),\n",
       " ('batch_normalization/beta:0', True),\n",
       " ('batch_normalization/moving_mean:0', False),\n",
       " ('batch_normalization/moving_variance:0', False)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn1 = model.layers[1]\n",
    "[(var.name, var.trainable) for var in bn1.variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 1.1207 - accuracy: 0.6292 - val_loss: 0.5539 - val_accuracy: 0.8160\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.5954 - accuracy: 0.7947 - val_loss: 0.4792 - val_accuracy: 0.8378\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.5296 - accuracy: 0.8168 - val_loss: 0.4424 - val_accuracy: 0.8494\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.4855 - accuracy: 0.8301 - val_loss: 0.4212 - val_accuracy: 0.8570\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.4683 - accuracy: 0.8354 - val_loss: 0.4051 - val_accuracy: 0.8616\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.4399 - accuracy: 0.8435 - val_loss: 0.3931 - val_accuracy: 0.8634\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.4263 - accuracy: 0.8507 - val_loss: 0.3829 - val_accuracy: 0.8640\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.4092 - accuracy: 0.8547 - val_loss: 0.3759 - val_accuracy: 0.8662\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.4093 - accuracy: 0.8550 - val_loss: 0.3692 - val_accuracy: 0.8678\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.3900 - accuracy: 0.8616 - val_loss: 0.3631 - val_accuracy: 0.8668\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_set_X, train_set_Y,\n",
    "                    epochs=10,\n",
    "                    validation_data=(valid_set_X, valid_set_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Clipping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another popular technique to lessen the exploding gradients problem is to simply clip the gradients during backpropagation so that they never exceed some threshold. This is called <b>Gradient Clipping</b>. This technique is most often used in recurrent neural networks, as Batch Normalization is tricky to use in RNNs.<br>\n",
    "All Keras optimizers accept `clipnorm` or `clipvalue` arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(clipvalue=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will clip every component of the gradient vector to a value between –1.0 and 1.0. This means that all the partial derivatives of the loss (with regards to each and every trainable parameter) will be clipped between –1.0 and 1.0. Note that it may change the orientation of the gradient vector. If you want to ensure that Gradient Clipping does not change the direction of the gradient vector, you should clip by norm by setting clipnorm instead of clipvalue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(clipnorm=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is generally not a good idea to train a very large DNN from scratch. Instead, you should always try to find an existing neural network that accomplishes a similar task to the one you are trying to tackle, then just reuse the lower layers of this network. This is called <b>Transfer Learning</b>. It will not only speed up training considerably, but will also require much less training data.<br>\n",
    "\n",
    "The output layer of the original model should usually be replaced since it is most likely not useful at all for the new task, and it may not even have the right number of outputs for the new task. Similarly, the upper hidden layers of the original model are less likely to be as useful as the lower layers, since the high-level features that are most useful for the new task\n",
    "may differ significantly from the ones that were most useful for the original task. You want to find the right number of layers to reuse. The more similar the tasks are, the more layers you want to reuse (starting with the lower layers). For very similar tasks, you can try keeping all the hidden layers and just replace the output layer.<br>\n",
    "\n",
    "Try freezing all the reused layers first (i.e., make their weights non-trainable, so gradient descent won’t modify them), then train your model and see how it performs. Then try unfreezing one or two of the top hidden layers to let backpropagation tweak\n",
    "them and see if performance improves. The more training data you have, the more layers you can unfreeze. It is also useful to reduce the learning rate when you unfreeze reused layers, this will avoid wrecking their fine-tuned weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reusing a Keras model\n",
    "\n",
    "Let's split the fashion MNIST training set in two:\n",
    "* `X_train_A`: all images of all items except for sandals and shirts (classes 5 and 6).\n",
    "* `X_train_B`: a much smaller training set of just the first 200 images of sandals or shirts.\n",
    "\n",
    "The validation set and the test set are also split this way, but without restricting the number of images.\n",
    "\n",
    "We will train a model on set A (classification task with 8 classes), and try to reuse it to tackle set B (binary classification). We hope to transfer a little bit of knowledge from task A to task B, since classes in set A (sneakers, ankle boots, coats, t-shirts, etc.) are somewhat similar to classes in set B (sandals and shirts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(X, y):\n",
    "    y_5_or_6 = (y == 5) | (y == 6) # sandals or shirts\n",
    "    y_A = y[~y_5_or_6]\n",
    "    y_A[y_A > 6] -= 2 # class indices 7, 8, 9 should be moved to 5, 6, 7\n",
    "    y_B = (y[y_5_or_6] == 6).astype(np.float32) # binary classification task: is it a shirt (class 6)?\n",
    "    return ((X[~y_5_or_6], y_A),\n",
    "            (X[y_5_or_6], y_B))\n",
    "\n",
    "(train_set_X_A, train_set_Y_A), (train_set_X_B, train_set_Y_B) = split_dataset(train_set_X, train_set_Y)\n",
    "(valid_set_X_A, valid_set_Y_A), (valid_set_X_B, valid_set_Y_B) = split_dataset(valid_set_X, valid_set_Y)\n",
    "(test_set_X_A, test_set_Y_A), (test_set_X_B, test_set_Y_B) = split_dataset(test_set_X, test_set_Y)\n",
    "train_set_X_B = train_set_X_B[:200]\n",
    "train_set_Y_B = train_set_Y_B[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43986, 28, 28)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_X_A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 28, 28)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_X_B.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 0, 5, 7, 7, 7, 4, 4, 3, 4, 0, 1, 6, 3, 4, 3, 2, 6, 5, 3],\n",
       "      dtype=uint8)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_Y_A[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 1.], dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_Y_B[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A = keras.models.Sequential()\n",
    "model_A.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "for n_hidden in (300, 100, 50, 50, 50):\n",
    "    model_A.add(keras.layers.Dense(n_hidden, activation=\"selu\"))\n",
    "model_A.add(keras.layers.Dense(8, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.9248 - accuracy: 0.6994 - val_loss: 0.3895 - val_accuracy: 0.8665\n",
      "Epoch 2/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.3651 - accuracy: 0.8745 - val_loss: 0.3287 - val_accuracy: 0.8824\n",
      "Epoch 3/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.3182 - accuracy: 0.8895 - val_loss: 0.3014 - val_accuracy: 0.8994\n",
      "Epoch 4/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.3049 - accuracy: 0.8956 - val_loss: 0.2894 - val_accuracy: 0.9021\n",
      "Epoch 5/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2804 - accuracy: 0.9026 - val_loss: 0.2775 - val_accuracy: 0.9061\n",
      "Epoch 6/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2701 - accuracy: 0.9076 - val_loss: 0.2735 - val_accuracy: 0.9063\n",
      "Epoch 7/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2626 - accuracy: 0.9096 - val_loss: 0.2721 - val_accuracy: 0.9083\n",
      "Epoch 8/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2610 - accuracy: 0.9122 - val_loss: 0.2589 - val_accuracy: 0.9136\n",
      "Epoch 9/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2558 - accuracy: 0.9107 - val_loss: 0.2562 - val_accuracy: 0.9143\n",
      "Epoch 10/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2511 - accuracy: 0.9137 - val_loss: 0.2543 - val_accuracy: 0.9155\n",
      "Epoch 11/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2431 - accuracy: 0.9172 - val_loss: 0.2497 - val_accuracy: 0.9158\n",
      "Epoch 12/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2422 - accuracy: 0.9171 - val_loss: 0.2514 - val_accuracy: 0.9128\n",
      "Epoch 13/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2360 - accuracy: 0.9179 - val_loss: 0.2447 - val_accuracy: 0.9160\n",
      "Epoch 14/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2266 - accuracy: 0.9232 - val_loss: 0.2416 - val_accuracy: 0.9178\n",
      "Epoch 15/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2225 - accuracy: 0.9240 - val_loss: 0.2448 - val_accuracy: 0.9188\n",
      "Epoch 16/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2260 - accuracy: 0.9214 - val_loss: 0.2386 - val_accuracy: 0.9195\n",
      "Epoch 17/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2191 - accuracy: 0.9253 - val_loss: 0.2407 - val_accuracy: 0.9183\n",
      "Epoch 18/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2171 - accuracy: 0.9252 - val_loss: 0.2432 - val_accuracy: 0.9153\n",
      "Epoch 19/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2181 - accuracy: 0.9245 - val_loss: 0.2332 - val_accuracy: 0.9208\n",
      "Epoch 20/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2113 - accuracy: 0.9276 - val_loss: 0.2333 - val_accuracy: 0.9200\n"
     ]
    }
   ],
   "source": [
    "history = model_A.fit(train_set_X_A, train_set_Y_A,\n",
    "                      epochs=20,\n",
    "                      validation_data=(valid_set_X_A, valid_set_Y_A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A.save(\"my_model_A.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to load `model_A`, and create a new model based on the `model_A`’s layers. But if two model share some layers, when we train one, it will also affect the other one. If we want to avoid that, we need to clone `model_A` before reusing its layers. To do this, we must clone `model_A`’s architecture, then copy its weights (since clone_model() does not clone the weights):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A = keras.models.load_model(\"my_model_A.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A_clone = keras.models.clone_model(model_A)\n",
    "model_A_clone.set_weights(model_A.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s reuse all layers except for the output layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_B_using_A = keras.models.Sequential(model_A_clone.layers[:-1])\n",
    "model_B_using_A.add(keras.layers.Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we could just train `model_B_using_A` for task B, but since the new output layer was initialized randomly, it will make large errors, at least during the first few epochs, so there will be large error gradients that may wreck the reused weights. To avoid this, one approach is to freeze the reused layers during the first few epochs, giving the new layer some time to learn reasonable weights. To do this, simply set every layer’s train able attribute to False and compile the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model_B_using_A.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model_B_using_A.compile(loss=\"binary_crossentropy\",\n",
    "                     optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "                     metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Note</b>: We must always compile our model after we freeze or unfreeze layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can train the model for a few epochs, then unfreeze the reused layers (which requires compiling the model again) and continue training to fine-tune the reused layers for task B. After unfreezing the reused layers, it is usually a good idea to reduce the learning rate, once again to avoid damaging the reused weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "7/7 [==============================] - 1s 38ms/step - loss: 0.6170 - accuracy: 0.6184 - val_loss: 0.5860 - val_accuracy: 0.6318\n",
      "Epoch 2/4\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 0.5562 - accuracy: 0.6583 - val_loss: 0.5482 - val_accuracy: 0.6704\n",
      "Epoch 3/4\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.4902 - accuracy: 0.7509 - val_loss: 0.5160 - val_accuracy: 0.7069\n",
      "Epoch 4/4\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 0.4903 - accuracy: 0.7405 - val_loss: 0.4871 - val_accuracy: 0.7292\n"
     ]
    }
   ],
   "source": [
    "history = model_B_using_A.fit(train_set_X_B, train_set_Y_B,\n",
    "                           epochs=4,\n",
    "                           validation_data=(valid_set_X_B, valid_set_Y_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model_B_using_A.layers[:-1]:\n",
    "    layer.trainable = True\n",
    "\n",
    "model_B_using_A.compile(loss=\"binary_crossentropy\",\n",
    "                     optimizer=keras.optimizers.SGD(lr=1e-4), # previous lr was 1e-3\n",
    "                     metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "7/7 [==============================] - 1s 37ms/step - loss: 0.4760 - accuracy: 0.7414 - val_loss: 0.4692 - val_accuracy: 0.7465\n",
      "Epoch 2/16\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.4404 - accuracy: 0.7944 - val_loss: 0.4505 - val_accuracy: 0.7657\n",
      "Epoch 3/16\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.3965 - accuracy: 0.8329 - val_loss: 0.4337 - val_accuracy: 0.7769\n",
      "Epoch 4/16\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.4076 - accuracy: 0.8226 - val_loss: 0.4185 - val_accuracy: 0.7901\n",
      "Epoch 5/16\n",
      "7/7 [==============================] - 0s 35ms/step - loss: 0.3590 - accuracy: 0.8701 - val_loss: 0.4037 - val_accuracy: 0.8063\n",
      "Epoch 6/16\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.3385 - accuracy: 0.8659 - val_loss: 0.3896 - val_accuracy: 0.8245\n",
      "Epoch 7/16\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 0.3724 - accuracy: 0.8369 - val_loss: 0.3772 - val_accuracy: 0.8327\n",
      "Epoch 8/16\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.3576 - accuracy: 0.8565 - val_loss: 0.3654 - val_accuracy: 0.8469\n",
      "Epoch 9/16\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.3242 - accuracy: 0.8953 - val_loss: 0.3533 - val_accuracy: 0.8590\n",
      "Epoch 10/16\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.3164 - accuracy: 0.8999 - val_loss: 0.3434 - val_accuracy: 0.8671\n",
      "Epoch 11/16\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.3038 - accuracy: 0.9105 - val_loss: 0.3335 - val_accuracy: 0.8732\n",
      "Epoch 12/16\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.3230 - accuracy: 0.8941 - val_loss: 0.3241 - val_accuracy: 0.8803\n",
      "Epoch 13/16\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.2826 - accuracy: 0.9229 - val_loss: 0.3147 - val_accuracy: 0.8864\n",
      "Epoch 14/16\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.2733 - accuracy: 0.9393 - val_loss: 0.3069 - val_accuracy: 0.8915\n",
      "Epoch 15/16\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.2709 - accuracy: 0.9372 - val_loss: 0.2988 - val_accuracy: 0.8966\n",
      "Epoch 16/16\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.2574 - accuracy: 0.9583 - val_loss: 0.2913 - val_accuracy: 0.9026\n"
     ]
    }
   ],
   "source": [
    "history = model_B_using_A.fit(train_set_X_B, train_set_Y_B,\n",
    "                           epochs=16,\n",
    "                           validation_data=(valid_set_X_B, valid_set_Y_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 1ms/step - loss: 0.2902 - accuracy: 0.9070\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.29017695784568787, 0.9070000052452087]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_B_using_A.evaluate(test_set_X_B, test_set_Y_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faster Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a very large deep neural network can be painfully slow. So far we have seen four ways to speed up training (and reach a better solution):<br>\n",
    "\n",
    "1. Applying a good initialization strategy for the connection weights\n",
    "2. Using a good activation function\n",
    "3. Using Batch Normalization\n",
    "4. Reusing parts of a pretrained network (possibly built on an auxiliary task or using unsupervised learning).<br>\n",
    "\n",
    "Another huge speed boost comes from:\n",
    "\n",
    "5. Using a faster optimizer than the regular Gradient Descent optimizer.<br>\n",
    "\n",
    "In this section we will present the most popular ones: Momentum optimization, Nesterov Accelerated\n",
    "Gradient, AdaGrad, RMSProp, and finally Adam and Nadam optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mometnum\n",
    "\n",
    "Imagine a bowling ball rolling down a gentle slope on a smooth surface: it will start out slowly, but it will quickly pick up momentum until it eventually reaches terminal velocity. This is the very simple idea behind `Momentum Optimization`. In contrast, regular Gradient Descent will simply take small regular steps down the slope, so it will take much more time to reach the bottom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nesterov Accelerated Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of `Nesterov Momentum optimization`, or `Nesterov Accelerated Gradient (NAG)`, is to measure the gradient of the cost function not at the local position but slightly ahead in the direction of the momentum. This small tweak works because in general the momentum vector will be pointing in the right direction (i.e., toward the optimum), so it will be slightly more accurate to use the gradient measured a bit farther in that direction rather than using the gradient at the original position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorithm decays the learning rate, but it does so faster for steep dimensions than for dimensions with gentler slopes. This is called an adaptive learning rate. It helps point the resulting updates more directly toward the global optimum.<br>\n",
    "\n",
    "AdaGrad often performs well for simple quadratic problems, but unfortunately it often stops too early when training neural networks. The learning rate gets scaled down so much that the algorithm ends up stopping entirely before reaching the\n",
    "global optimum. So even though Keras has an Adagrad optimizer, you should not use it to train deep neural networks (it may be efficient for simpler tasks such as Linear Regression, though). However, understanding Adagrad is helpful to grasp the other\n",
    "adaptive learning rate optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adagrad(lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSProp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although AdaGrad slows down a bit too fast and ends up never converging to the global optimum, the RMSProp algorithm fixes this by accumulating only the gradients from the most recent iterations (as opposed to all the gradients since the beginning\n",
    "of training). It does so by using exponential decay in the first step.<br>\n",
    "\n",
    "Except on very simple problems, this optimizer almost always performs much better than AdaGrad. In fact, it was the preferred optimization algorithm of many researchers until Adam optimization came around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam, which stands for <b>adaptive moment estimation</b>, combines the ideas of Momentum optimization and RMSProp: just like Momentum optimization it keeps track of an exponentially decaying average of past gradients, and just like RMSProp it keeps track of an exponentially decaying average of past squared gradients.\n",
    "\n",
    "Since Adam is an adaptive learning rate algorithm (like AdaGrad and RMSProp), it requires less tuning of the learning rate hyperparameter. You can often use the default value lr = 0.001, making Adam even easier to use than Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, two variants of Adam are worth mentioning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adamax\n",
    "\n",
    "while Adam accumulates the squares of the gradients, Adamax scales down the gradient updates by a factor of s, which is just the max of the time-decayed gradients.<br>\n",
    "\n",
    "In practice, this can make Adamax more stable than Adam, but this really depends on the dataset, and in general Adam actually performs better. So it’s just one more optimizer you can try if you experience problems with Adam on some task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adamax(lr=0.001, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nadam\n",
    "\n",
    "Nadam optimization is more important than Adamax. It is simply Adam optimization plus the Nesterov trick, so it will often converge slightly faster than Adam. Nadam generally outperforms Adam, but is sometimes outperformed by RMSProp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Nadam(lr=0.001, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Note:</b> Adaptive optimization methods (including RMSProp, Adam and Nadam optimization) are often great, converging fast to a good solution. However, they can lead to solutions that generalize poorly on some datasets. So when you are disappointed by your model’s performance, try using plain Nesterov Accelerated Gradient instead, because your dataset may just be allergic to adaptive gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avoiding Overfitting Through Regularization\n",
    "\n",
    "We already implemented one of the best regularization techniques in Chapter 10, early stopping.<br>\n",
    "Moreover, even though Batch Normalization was designed to solve the vanishing/exploding gradients problems, is also acts like a pretty good regularizer. In this section we will present other popular regularization techniques for neural networks:\n",
    "ℓ1 and ℓ2 regularization, dropout and max-norm regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\ell_1$ and $\\ell_2$ Regularization\n",
    "\n",
    "The main intuitive difference between the L1 and L2 regularization is that L1 regularization tries to estimate the median of the data while the L2 regularization tries to estimate the mean of the data to avoid overfitting.<br>\n",
    "\n",
    "Both functions return a regularizer that will be called to compute the regularization loss, at each step during training. This regularization loss is then added to the final loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = keras.layers.Dense(100, activation=\"elu\",\n",
    "                           kernel_initializer=\"he_normal\",\n",
    "                           kernel_regularizer=keras.regularizers.l2(0.01))\n",
    "# or l1(0.1) for ℓ1 regularization with a factor or 0.1\n",
    "# or l1_l2(0.1, 0.01) for both ℓ1 and ℓ2 regularization, with factors 0.1 and 0.01 respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"elu\",\n",
    "                       kernel_initializer=\"he_normal\",\n",
    "                       kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "    keras.layers.Dense(100, activation=\"elu\",\n",
    "                       kernel_initializer=\"he_normal\",\n",
    "                       kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "    keras.layers.Dense(10, activation=\"softmax\",\n",
    "                       kernel_regularizer=keras.regularizers.l2(0.01))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 3.2647 - accuracy: 0.7944 - val_loss: 0.7198 - val_accuracy: 0.8320\n",
      "Epoch 2/2\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.7275 - accuracy: 0.8249 - val_loss: 0.6808 - val_accuracy: 0.8426\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_set_X_scaled, train_set_Y,\n",
    "                    epochs=2,\n",
    "                    validation_data=(valid_set_X_scaled, valid_set_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "\n",
    "Dropout is one of the most popular regularization techniques for deep neural networks, and it has proven to be highly successful.<br>\n",
    "\n",
    "It is a fairly simple algorithm. At every training step, every neuron (including the input neurons, but always excluding the output neurons) has a probability p of being temporarily “dropped out,” meaning it will be entirely ignored during this training\n",
    "step, but it may be active during the next step.<br>\n",
    "\n",
    "Neurons trained with dropout cannot co-adapt with their neighboring neurons. They have to be as useful as possible on their own. They also cannot rely excessively on just a few input neurons. They must pay attention to each of their input neurons. They end up being less sensitive to slight changes in the inputs. In the end you get a more robust network that generalizes better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.7265 - accuracy: 0.7629 - val_loss: 0.3661 - val_accuracy: 0.8682\n",
      "Epoch 2/2\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.4261 - accuracy: 0.8424 - val_loss: 0.3430 - val_accuracy: 0.8736\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_set_X_scaled, train_set_Y,\n",
    "                    epochs=2,\n",
    "                    validation_data=(valid_set_X_scaled, valid_set_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alpha Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to regularize a self-normalizing network based on the SELU activation function, you should use AlphaDropout. This is a variant of dropout that preserves the mean and standard deviation of its inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.AlphaDropout(rate=0.2),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.AlphaDropout(rate=0.2),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.AlphaDropout(rate=0.2),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.8023 - accuracy: 0.7146 - val_loss: 0.5778 - val_accuracy: 0.8446\n",
      "Epoch 2/20\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.5662 - accuracy: 0.7904 - val_loss: 0.5146 - val_accuracy: 0.8528\n",
      "Epoch 3/20\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.5260 - accuracy: 0.8062 - val_loss: 0.4878 - val_accuracy: 0.8604\n",
      "Epoch 4/20\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.5125 - accuracy: 0.8098 - val_loss: 0.4810 - val_accuracy: 0.8576\n",
      "Epoch 5/20\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.5074 - accuracy: 0.8126 - val_loss: 0.4250 - val_accuracy: 0.8686\n",
      "Epoch 6/20\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.4789 - accuracy: 0.8201 - val_loss: 0.4600 - val_accuracy: 0.8640\n",
      "Epoch 7/20\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.4721 - accuracy: 0.8273 - val_loss: 0.4672 - val_accuracy: 0.8624\n",
      "Epoch 8/20\n",
      "1719/1719 [==============================] - 8s 4ms/step - loss: 0.4570 - accuracy: 0.8300 - val_loss: 0.4191 - val_accuracy: 0.8678\n",
      "Epoch 9/20\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.4625 - accuracy: 0.8292 - val_loss: 0.4328 - val_accuracy: 0.8736\n",
      "Epoch 10/20\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.4548 - accuracy: 0.8308 - val_loss: 0.4364 - val_accuracy: 0.8662\n",
      "Epoch 11/20\n",
      "1719/1719 [==============================] - 9s 6ms/step - loss: 0.4459 - accuracy: 0.8344 - val_loss: 0.4409 - val_accuracy: 0.8708\n",
      "Epoch 12/20\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.4412 - accuracy: 0.8367 - val_loss: 0.5234 - val_accuracy: 0.8564\n",
      "Epoch 13/20\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.4324 - accuracy: 0.8388 - val_loss: 0.4335 - val_accuracy: 0.8746\n",
      "Epoch 14/20\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.4298 - accuracy: 0.8380 - val_loss: 0.4388 - val_accuracy: 0.8662\n",
      "Epoch 15/20\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.4315 - accuracy: 0.8373 - val_loss: 0.4256 - val_accuracy: 0.8694\n",
      "Epoch 16/20\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.4257 - accuracy: 0.8409 - val_loss: 0.4069 - val_accuracy: 0.8778\n",
      "Epoch 17/20\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.4200 - accuracy: 0.8425 - val_loss: 0.5395 - val_accuracy: 0.8566\n",
      "Epoch 18/20\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.4345 - accuracy: 0.8388 - val_loss: 0.4622 - val_accuracy: 0.8684\n",
      "Epoch 19/20\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.4277 - accuracy: 0.8422 - val_loss: 0.4648 - val_accuracy: 0.8702\n",
      "Epoch 20/20\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.4194 - accuracy: 0.8425 - val_loss: 0.4405 - val_accuracy: 0.8728\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_set_X_scaled, train_set_Y,\n",
    "                    epochs=20,\n",
    "                    validation_data=(valid_set_X_scaled, valid_set_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max-Norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another regularization technique that is quite popular for neural networks is called max-norm regularization. For each neuron, it constrains the weights w of the incoming connections such that ∥ *w* ∥2 ≤ _r_, where r is the max-norm hyperparameter\n",
    "and ∥ · ∥2 is the ℓ2 norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                       kernel_constraint=keras.constraints.max_norm(1.)),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                       kernel_constraint=keras.constraints.max_norm(1.)),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1719/1719 [==============================] - 8s 4ms/step - loss: 0.5767 - accuracy: 0.8034 - val_loss: 0.3728 - val_accuracy: 0.8628\n",
      "Epoch 2/2\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.3530 - accuracy: 0.8694 - val_loss: 0.3780 - val_accuracy: 0.8614\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_set_X_scaled, train_set_Y,\n",
    "                    epochs=2,\n",
    "                    validation_data=(valid_set_X_scaled, valid_set_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final suggestions\n",
    "\n",
    "• If your model self-normalizes:<br>\n",
    "If it overfits the training set, then you should add alpha dropout (and always use early stopping as well). Do not use other regularization methods, or else they would break self-normalization.<br>\n",
    "\n",
    "• If your model cannot self-normalize (e.g., it is a recurrent net or it contains skip connections):<br>\n",
    "You can try using ELU (or another activation function) instead of SELU, it may perform better. Make sure to change the initialization method accordingly (e.g., He init for ELU or ReLU).<br>\n",
    "If it is a deep network, you should use Batch Normalization after every hidden layer. If it overfits the training set, you can also try using max-norm or ℓ2 regularization.<br>\n",
    "\n",
    "• If you need a sparse model:<br>\n",
    "You can use ℓ1 regularization (and optionally zero out the tiny weights after training). If you need an even sparser model, you can try using FTRL instead of Nadam optimization, along with ℓ1 regularization. In any case, this will break self-normalization, so you will need to switch to BN if your model is deep.<br>\n",
    "\n",
    "• If you need a low-latency model (one that performs lightning-fast predictions):<br>\n",
    "You may need to use less layers, avoid Batch Normalization, and possibly replace the SELU activation function with the leaky ReLU. Having a sparse model will also help. You may also want to reduce the float precision from 32-bits to 16-bit\n",
    "(or even 8-bits)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Jupyter Projects",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
